{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train=pd.read_csv('/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv')\n",
    "df_test=pd.read_csv('/Users/rishav/Downloads/final_test_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH_ID                                                                                       0\n",
      "Sector                                                                                      0\n",
      "State                                                                                       0\n",
      "NSS-Region                                                                                  0\n",
      "District                                                                                    0\n",
      "Household Type                                                                              0\n",
      "Religion of the head of the household                                                       0\n",
      "Social Group of the head of the household                                                   0\n",
      "HH Size (For FDQ)                                                                           0\n",
      "NCO_3D                                                                                      0\n",
      "NIC_5D                                                                                      0\n",
      "Is_online_Clothing_Purchased_Last365                                                        0\n",
      "Is_online_Footwear_Purchased_Last365                                                        0\n",
      "Is_online_Furniture_fixturesPurchased_Last365                                               0\n",
      "Is_online_Mobile_Handset_Purchased_Last365                                                  0\n",
      "Is_online_Personal_Goods_Purchased_Last365                                                  0\n",
      "Is_online_Recreation_Goods_Purchased_Last365                                                0\n",
      "Is_online_Household_Appliances_Purchased_Last365                                            0\n",
      "Is_online_Crockery_Utensils_Purchased_Last365                                               0\n",
      "Is_online_Sports_Goods_Purchased_Last365                                                    0\n",
      "Is_online_Medical_Equipment_Purchased_Last365                                               0\n",
      "Is_online_Bedding_Purchased_Last365                                                         0\n",
      "Is_HH_Have_Television                                                                       0\n",
      "Is_HH_Have_Radio                                                                            0\n",
      "Is_HH_Have_Laptop_PC                                                                        0\n",
      "Is_HH_Have_Mobile_handset                                                                   0\n",
      "Is_HH_Have_Bicycle                                                                          0\n",
      "Is_HH_Have_Motorcycle_scooter                                                               0\n",
      "Is_HH_Have_Motorcar_jeep_van                                                                0\n",
      "Is_HH_Have_Trucks                                                                           0\n",
      "Is_HH_Have_Animal_cart                                                                      0\n",
      "Is_HH_Have_Refrigerator                                                                     0\n",
      "Is_HH_Have_Washing_machine                                                                  0\n",
      "Is_HH_Have_Airconditioner_aircooler                                                         0\n",
      "TotalExpense                                                                                0\n",
      "person_count                                                                                0\n",
      "avg_age                                                                                     0\n",
      "max_age                                                                                     0\n",
      "min_age                                                                                     0\n",
      "gender_1_count                                                                              0\n",
      "gender_2_count                                                                              0\n",
      "gender_3_count                                                                              0\n",
      "avg_education                                                                               0\n",
      "max_education                                                                               0\n",
      "No. of meals usually taken in a day_sum                                                     0\n",
      "No. of meals usually taken in a day_mean                                                    0\n",
      "No. of meals taken during last 30 days from school, balwadi etc._sum                        0\n",
      "No. of meals taken during last 30 days from school, balwadi etc._mean                       0\n",
      "No. of meals taken during last 30 days from employer as perquisites or part of wage_sum     0\n",
      "No. of meals taken during last 30 days from employer as perquisites or part of wage_mean    0\n",
      "No. of meals taken during last 30 days on payment_sum                                       0\n",
      "No. of meals taken during last 30 days on payment_mean                                      0\n",
      "No. of meals taken during last 30 days at home_sum                                          0\n",
      "No. of meals taken during last 30 days at home_mean                                         0\n",
      "internet_users_count                                                                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test[['NCO_3D', 'NIC_5D']] = df_test[['NCO_3D', 'NIC_5D']].fillna(1000)\n",
    "df_test.to_csv(\"test_data_final.csv\", index=False)\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "(225316, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/rishav/Downloads/test_data_final.csv')\n",
    "\n",
    "# Find columns with missing values\n",
    "null_columns = df.columns[df.isnull().sum() > 0]\n",
    "\n",
    "# Print the count of missing values in each column with missing data\n",
    "print(df.isnull().sum()[null_columns])\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters: 7\n",
      "\n",
      "--- Cluster-wise XGBoost Prediction ---\n",
      "Mean Percentage Error (MPE): 26.54%\n",
      "Mean Absolute Error (MAE): 1189.40\n",
      "R² Score: 0.6828\n",
      "\n",
      "Predictions saved to test_data_with_predictions_xgb.csv\n",
      "\n",
      "Sample Predictions:\n",
      "   predicted_expense\n",
      "0        6905.832031\n",
      "1        4735.216309\n",
      "2        4684.315430\n",
      "3        3770.371582\n",
      "4        9016.327148\n",
      "Saved data with household sums to test_data_with_household_sums_xgb.csv\n",
      "\n",
      "Sample rows from household HCES2022655561010131113011101202304:\n",
      "                                 HH_ID  predicted_expense\n",
      "0  HCES2022655561010131113011101202304       20095.735352\n",
      "1  HCES2022655561010131113011101202304       20095.735352\n",
      "2  HCES2022655561010131113011101202304       20095.735352\n",
      "3  HCES2022655561010131113011101202304       20095.735352\n",
      "Number of households for comparison: 52350\n",
      "\n",
      "----- Evaluation Metrics -----\n",
      "R² Score: 0.5590\n",
      "Mean Squared Error (MSE): 82091652.47\n",
      "Root Mean Squared Error (RMSE): 9060.44\n",
      "Mean Absolute Error (MAE): 5252.16\n",
      "Mean Percentage Error (MPE): 27.58%\n",
      "Mean Absolute Percentage Error (MAPE): 27.58%\n",
      "\n",
      "Final evaluation metrics:\n",
      "r2: 0.559001963565406\n",
      "mse: 82091652.46941583\n",
      "rmse: 9060.44438586849\n",
      "mae: 5252.159923224689\n",
      "mpe: 27.58133406002838\n",
      "mape: 27.58133406002838\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# --- Standardize numeric features ---\n",
    "def standardize_features(file_path, target_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    if target_column in non_numeric_cols:\n",
    "        non_numeric_cols.remove(target_column)\n",
    "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_numeric)\n",
    "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist()\n",
    "\n",
    "# --- Elbow method to determine optimal clusters ---\n",
    "def elbow_method_auto(X, max_clusters=10):\n",
    "    inertias = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deltas = np.diff(deltas)\n",
    "    elbow_point = np.argmin(second_deltas) + 2  # Adjust for index shift\n",
    "    return elbow_point\n",
    "\n",
    "# --- Evaluation metrics ---\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
    "    mpe = mean_percentage_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {method_name} ---\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# --- Main Training Function with XGBoost ---\n",
    "def cluster_and_predict_xgb(file_path, target_column='WeightedExpense', max_clusters=10):\n",
    "    # Standardize\n",
    "    X_scaled, y, scaler, non_numeric_cols, numeric_cols = standardize_features(file_path, target_column)\n",
    "\n",
    "    # Determine optimal number of clusters\n",
    "    optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Cluster-wise XGBoost model training\n",
    "    preds_model = np.zeros_like(y)\n",
    "    cluster_models = {}\n",
    "\n",
    "    # XGBoost parameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        X_cluster = X_scaled[cluster_idx]\n",
    "        y_cluster = y[cluster_idx]\n",
    "\n",
    "        # Skip empty clusters (shouldn't happen but just in case)\n",
    "        if len(y_cluster) == 0:\n",
    "            continue\n",
    "\n",
    "        # Train XGBoost model for this cluster\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        model.fit(X_cluster, y_cluster)\n",
    "        \n",
    "        # Make predictions for this cluster\n",
    "        preds_model[cluster_idx] = model.predict(X_cluster)\n",
    "\n",
    "        # Save model per cluster\n",
    "        cluster_models[cluster_id] = model\n",
    "\n",
    "    # Evaluate training performance\n",
    "    evaluate_predictions(y, preds_model, method_name=\"Cluster-wise XGBoost Prediction\")\n",
    "\n",
    "    # Return trained components for prediction\n",
    "    return scaler, kmeans, cluster_models, numeric_cols, non_numeric_cols\n",
    "\n",
    "def predict_on_test_data(test_csv_path, scaler, kmeans, cluster_models, numeric_cols, non_numeric_cols, target_column='WeightedExpense'):\n",
    "    \"\"\"\n",
    "    Predict expenses on test data using trained XGBoost models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_csv_path : str\n",
    "        Path to the test CSV file\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler from training data\n",
    "    kmeans : KMeans\n",
    "        Trained KMeans model\n",
    "    cluster_models : dict\n",
    "        Dictionary of XGBoost models for each cluster\n",
    "    numeric_cols : list\n",
    "        List of numeric column names used in training\n",
    "    non_numeric_cols : list\n",
    "        List of non-numeric column names from training\n",
    "    target_column : str\n",
    "        Name of the target column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Test data with predictions\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Drop 'TotalExpense' if present\n",
    "    if 'TotalExpense' in test_data.columns:\n",
    "        test_data = test_data.drop(columns=['TotalExpense'])\n",
    "    \n",
    "    # Check if all required numeric columns exist in test data\n",
    "    missing_cols = [col for col in numeric_cols if col not in test_data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Test data is missing columns that were used in training: {missing_cols}\")\n",
    "    \n",
    "    # Create a DataFrame with only the numeric columns in the correct order\n",
    "    X_numeric_test = pd.DataFrame(index=test_data.index)\n",
    "    for col in numeric_cols:\n",
    "        X_numeric_test[col] = test_data[col]\n",
    "    \n",
    "    # Handle missing values\n",
    "    if X_numeric_test.isna().any().any():\n",
    "        cols_with_nan = X_numeric_test.columns[X_numeric_test.isna().any()].tolist()\n",
    "        print(f\"Warning: NaN values found in columns: {cols_with_nan}\")\n",
    "        \n",
    "        # Fill NaN values with appropriate defaults for each column\n",
    "        for col in cols_with_nan:\n",
    "            print(f\"Filling NaN values in {col}\")\n",
    "            if col in ['NCO_3D', 'NIC_5D']:\n",
    "                X_numeric_test[col] = X_numeric_test[col].fillna(1000)\n",
    "            else:\n",
    "                X_numeric_test[col] = X_numeric_test[col].fillna(X_numeric_test[col].median())\n",
    "    \n",
    "    # Double-check for any remaining NaN values\n",
    "    if X_numeric_test.isna().any().any():\n",
    "        raise ValueError(\"Failed to handle all NaN values in numeric features\")\n",
    "        \n",
    "    # Standardize features using the same scaler from training\n",
    "    X_scaled_test = scaler.transform(X_numeric_test)\n",
    "    \n",
    "    # Predict clusters\n",
    "    test_clusters = kmeans.predict(X_scaled_test)\n",
    "    \n",
    "    # Predict using cluster-specific XGBoost models\n",
    "    predictions = np.zeros(X_scaled_test.shape[0])\n",
    "    \n",
    "    for cluster_id in np.unique(test_clusters):\n",
    "        cluster_idx = test_clusters == cluster_id\n",
    "        X_cluster_test = X_scaled_test[cluster_idx]\n",
    "        \n",
    "        if cluster_id in cluster_models:\n",
    "            model = cluster_models[cluster_id]\n",
    "            predictions[cluster_idx] = model.predict(X_cluster_test)\n",
    "        else:\n",
    "            print(f\"Warning: No model for cluster {cluster_id}, using average prediction\")\n",
    "            # Use average of all predictions for this cluster\n",
    "            cluster_means = np.mean([m.predict(X_cluster_test).mean() \n",
    "                                     for m in cluster_models.values()])\n",
    "            predictions[cluster_idx] = cluster_means\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    test_data['predicted_expense'] = predictions\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    output_path = 'test_data_with_predictions_xgb.csv'\n",
    "    test_data.to_csv(output_path, index=False)\n",
    "    print(f\"\\nPredictions saved to {output_path}\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(test_data[['predicted_expense']].head())\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "def aggregate_household_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by household ID\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the sum of predicted_expense for each household ID\n",
    "    household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
    "\n",
    "    # Step 2: Replace each row's predicted_expense with its household sum\n",
    "    test_results['predicted_expense'] = test_results['HH_ID'].map(household_sums)\n",
    "\n",
    "    # Step 3: Save the updated dataframe to CSV\n",
    "    output_path = 'test_data_with_household_sums_xgb.csv'\n",
    "    test_results.to_csv(output_path, index=False)\n",
    "    print(f\"Saved data with household sums to {output_path}\")\n",
    "\n",
    "    # Step 4: Verify the result with a sample household\n",
    "    sample_household = test_results['HH_ID'].iloc[0]\n",
    "    sample_rows = test_results[test_results['HH_ID'] == sample_household].head()\n",
    "    print(f\"\\nSample rows from household {sample_household}:\")\n",
    "    print(sample_rows[['HH_ID', 'predicted_expense']])\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# --- Main execution ---\n",
    "# Training with XGBoost\n",
    "scaler, kmeans, cluster_models, numeric_cols, non_numeric_cols = cluster_and_predict_xgb(\n",
    "   '/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv',\n",
    "    target_column='WeightedExpense'\n",
    ")\n",
    "\n",
    "# Predict on Test CSV\n",
    "test_results = predict_on_test_data(\n",
    "    test_csv_path='/Users/rishav/Downloads/test_data_final.csv',\n",
    "    scaler=scaler,\n",
    "    kmeans=kmeans,\n",
    "    cluster_models=cluster_models,\n",
    "    numeric_cols=numeric_cols,\n",
    "    non_numeric_cols=non_numeric_cols,\n",
    "    target_column='WeightedExpense'\n",
    ")\n",
    "\n",
    "# Aggregate predictions by household\n",
    "final_results = aggregate_household_predictions(test_results)\n",
    "\n",
    "# Optional: Compare predictions with actual values if you have them\n",
    "# Define the comparison function directly here instead of trying to import it\n",
    "def compare_expenses(predictions_csv, actual_csv, \n",
    "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
    "                     join_col='HH_ID'):\n",
    "    \"\"\"\n",
    "    Compare predicted expenses with actual expenses and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    pred_df = pd.read_csv(predictions_csv)\n",
    "    actual_df = pd.read_csv(actual_csv)\n",
    "    \n",
    "    # Check if the datasets have the required columns\n",
    "    required_cols = {\n",
    "        'predictions': [pred_col, join_col],\n",
    "        'actual': [actual_col, join_col]\n",
    "    }\n",
    "    \n",
    "    for df_name, cols in required_cols.items():\n",
    "        df = pred_df if df_name == 'predictions' else actual_df\n",
    "        missing = [col for col in cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
    "    \n",
    "    # Merge the datasets\n",
    "    pred_unique = pred_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
    "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
    "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
    "    \n",
    "    # Print the number of households we can compare\n",
    "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
    "    \n",
    "    # Extract the values for comparison\n",
    "    y_pred = merged_df[pred_col].values\n",
    "    y_true = merged_df[actual_col].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Calculate Mean Percentage Error (avoiding division by zero)\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        mpe = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    else:\n",
    "        mpe = np.nan\n",
    "    \n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(1e-8, np.abs(y_true)))) * 100\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\n----- Evaluation Metrics -----\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'mape': mape\n",
    "    }\n",
    "\n",
    "# Check if test data has actual values\n",
    "if 'TotalExpense' in pd.read_csv('/Users/rishav/Downloads/final_test_dataset.csv').columns:\n",
    "    # Add the missing import\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    # Run comparison\n",
    "    metrics = compare_expenses(\n",
    "        predictions_csv='test_data_with_household_sums_xgb.csv',\n",
    "        actual_csv='/Users/rishav/Downloads/final_test_dataset.csv',\n",
    "        pred_col='predicted_expense',\n",
    "        actual_col='TotalExpense',\n",
    "        join_col='HH_ID'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal evaluation metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model directly on 901723 samples with 48 features\n",
      "\n",
      "--- XGBoost Prediction ---\n",
      "Mean Percentage Error (MPE): 26.10%\n",
      "Mean Absolute Error (MAE): 1180.08\n",
      "R² Score: 0.6777\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                                              Feature  Importance\n",
      "46                         Is_HH_Have_Washing_machine    0.173264\n",
      "38                               Is_HH_Have_Laptop_PC    0.106223\n",
      "0                                      Person Srl No.    0.105258\n",
      "45                            Is_HH_Have_Refrigerator    0.088647\n",
      "15                                             Sector    0.043663\n",
      "22                                  HH Size (For FDQ)    0.037203\n",
      "25               Is_online_Clothing_Purchased_Last365    0.036808\n",
      "42                       Is_HH_Have_Motorcar_jeep_van    0.031044\n",
      "7   Whether used internet from any location during...    0.029159\n",
      "1                             Relation to head (code)    0.025902\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Test data is missing columns that were used in training: ['Person Srl No.', 'Relation to head (code)', 'Gender', 'Age(in years)', 'Marital Status (code)', 'Highest educational level attained (code)', 'Total year of education completed', 'Whether used internet from any location during last 30 days', 'No. of days stayed away from home during last 30 days', 'No. of meals usually taken in a day', 'No. of meals taken during last 30 days from school, balwadi etc.', 'No. of meals taken during last 30 days from employer as perquisites or part of wage', 'No. of meals taken during last 30 days  others', 'No. of meals taken during last 30 days on payment', 'No. of meals taken during last 30 days at home']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 236\u001b[39m\n\u001b[32m    230\u001b[39m model, scaler, numeric_cols, non_numeric_cols = train_xgboost(\n\u001b[32m    231\u001b[39m    \u001b[33m'\u001b[39m\u001b[33m/Users/rishav/Downloads/modified_data.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    232\u001b[39m     target_column=\u001b[33m'\u001b[39m\u001b[33mWeightedExpense\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    233\u001b[39m )\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Predict on Test CSV\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m test_results = \u001b[43mpredict_on_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_csv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Users/rishav/Downloads/test_data_final.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnon_numeric_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_numeric_cols\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Aggregate predictions by household\u001b[39;00m\n\u001b[32m    245\u001b[39m final_results = aggregate_household_predictions(test_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mpredict_on_test_data\u001b[39m\u001b[34m(test_csv_path, model, scaler, numeric_cols, non_numeric_cols)\u001b[39m\n\u001b[32m     89\u001b[39m missing_cols = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_cols \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m test_data.columns]\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest data is missing columns that were used in training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Create a DataFrame with only the numeric columns in the correct order\u001b[39;00m\n\u001b[32m     94\u001b[39m X_numeric_test = pd.DataFrame(index=test_data.index)\n",
      "\u001b[31mValueError\u001b[39m: Test data is missing columns that were used in training: ['Person Srl No.', 'Relation to head (code)', 'Gender', 'Age(in years)', 'Marital Status (code)', 'Highest educational level attained (code)', 'Total year of education completed', 'Whether used internet from any location during last 30 days', 'No. of days stayed away from home during last 30 days', 'No. of meals usually taken in a day', 'No. of meals taken during last 30 days from school, balwadi etc.', 'No. of meals taken during last 30 days from employer as perquisites or part of wage', 'No. of meals taken during last 30 days  others', 'No. of meals taken during last 30 days on payment', 'No. of meals taken during last 30 days at home']"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# --- Standardize numeric features ---\n",
    "def standardize_features(file_path, target_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    if target_column in non_numeric_cols:\n",
    "        non_numeric_cols.remove(target_column)\n",
    "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_numeric)\n",
    "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist(), df\n",
    "\n",
    "# --- Evaluation metrics ---\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
    "    mpe = mean_percentage_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {method_name} ---\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# --- Direct XGBoost Training Function ---\n",
    "def train_xgboost(file_path, target_column='WeightedExpense'):\n",
    "    # Standardize\n",
    "    X_scaled, y, scaler, non_numeric_cols, numeric_cols, df = standardize_features(file_path, target_column)\n",
    "    \n",
    "    print(f\"Training XGBoost model directly on {len(y)} samples with {X_scaled.shape[1]} features\")\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "        'n_jobs': -1  # Use all cores\n",
    "    }\n",
    "    \n",
    "    # Train a single XGBoost model on all data\n",
    "    model = XGBRegressor(**xgb_params)\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Make predictions on training data\n",
    "    train_preds = model.predict(X_scaled)\n",
    "    \n",
    "    # Evaluate training performance\n",
    "    evaluate_predictions(y, train_preds, method_name=\"XGBoost Prediction\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Return trained components for prediction\n",
    "    return model, scaler, numeric_cols, non_numeric_cols\n",
    "\n",
    "def predict_on_test_data(test_csv_path, model, scaler, numeric_cols, non_numeric_cols):\n",
    "    \"\"\"\n",
    "    Predict expenses on test data using the trained XGBoost model.\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Drop 'TotalExpense' if present\n",
    "    if 'TotalExpense' in test_data.columns:\n",
    "        test_data = test_data.drop(columns=['TotalExpense'])\n",
    "    \n",
    "    # Check if all required numeric columns exist in test data\n",
    "    missing_cols = [col for col in numeric_cols if col not in test_data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Test data is missing columns that were used in training: {missing_cols}\")\n",
    "    \n",
    "    # Create a DataFrame with only the numeric columns in the correct order\n",
    "    X_numeric_test = pd.DataFrame(index=test_data.index)\n",
    "    for col in numeric_cols:\n",
    "        X_numeric_test[col] = test_data[col]\n",
    "    \n",
    "    # Handle missing values\n",
    "    if X_numeric_test.isna().any().any():\n",
    "        cols_with_nan = X_numeric_test.columns[X_numeric_test.isna().any()].tolist()\n",
    "        print(f\"Warning: NaN values found in columns: {cols_with_nan}\")\n",
    "        \n",
    "        # Fill NaN values with appropriate defaults for each column\n",
    "        for col in cols_with_nan:\n",
    "            print(f\"Filling NaN values in {col}\")\n",
    "            if col in ['NCO_3D', 'NIC_5D']:\n",
    "                X_numeric_test[col] = X_numeric_test[col].fillna(1000)\n",
    "            else:\n",
    "                X_numeric_test[col] = X_numeric_test[col].fillna(X_numeric_test[col].median())\n",
    "    \n",
    "    # Double-check for any remaining NaN values\n",
    "    if X_numeric_test.isna().any().any():\n",
    "        raise ValueError(\"Failed to handle all NaN values in numeric features\")\n",
    "        \n",
    "    # Standardize features using the same scaler from training\n",
    "    X_scaled_test = scaler.transform(X_numeric_test)\n",
    "    \n",
    "    # Predict using the XGBoost model\n",
    "    predictions = model.predict(X_scaled_test)\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    test_data['predicted_expense'] = predictions\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    output_path = 'test_data_with_predictions_xgb_direct.csv'\n",
    "    test_data.to_csv(output_path, index=False)\n",
    "    print(f\"\\nPredictions saved to {output_path}\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(test_data[['predicted_expense']].head())\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "def aggregate_household_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by household ID\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the sum of predicted_expense for each household ID\n",
    "    household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
    "\n",
    "    # Step 2: Replace each row's predicted_expense with its household sum\n",
    "    test_results['predicted_expense'] = test_results['HH_ID'].map(household_sums)\n",
    "\n",
    "    # Step 3: Save the updated dataframe to CSV\n",
    "    output_path = 'test_data_with_household_sums_xgb_direct.csv'\n",
    "    test_results.to_csv(output_path, index=False)\n",
    "    print(f\"Saved data with household sums to {output_path}\")\n",
    "\n",
    "    # Step 4: Verify the result with a sample household\n",
    "    sample_household = test_results['HH_ID'].iloc[0]\n",
    "    sample_rows = test_results[test_results['HH_ID'] == sample_household].head()\n",
    "    print(f\"\\nSample rows from household {sample_household}:\")\n",
    "    print(sample_rows[['HH_ID', 'predicted_expense']])\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Function to compare predictions with actual values\n",
    "def compare_expenses(predictions_csv, actual_csv, \n",
    "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
    "                     join_col='HH_ID'):\n",
    "    \"\"\"\n",
    "    Compare predicted expenses with actual expenses and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    pred_df = pd.read_csv(predictions_csv)\n",
    "    actual_df = pd.read_csv(actual_csv)\n",
    "    \n",
    "    # Check if the datasets have the required columns\n",
    "    required_cols = {\n",
    "        'predictions': [pred_col, join_col],\n",
    "        'actual': [actual_col, join_col]\n",
    "    }\n",
    "    \n",
    "    for df_name, cols in required_cols.items():\n",
    "        df = pred_df if df_name == 'predictions' else actual_df\n",
    "        missing = [col for col in cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
    "    \n",
    "    # Merge the datasets\n",
    "    pred_unique = pred_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
    "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
    "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
    "    \n",
    "    # Print the number of households we can compare\n",
    "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
    "    \n",
    "    # Extract the values for comparison\n",
    "    y_pred = merged_df[pred_col].values\n",
    "    y_true = merged_df[actual_col].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Calculate Mean Percentage Error (avoiding division by zero)\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        mpe = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    else:\n",
    "        mpe = np.nan\n",
    "    \n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(1e-8, np.abs(y_true)))) * 100\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\n----- Evaluation Metrics -----\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'mape': mape\n",
    "    }\n",
    "\n",
    "# --- Main execution ---\n",
    "# Training with direct XGBoost (no clustering)\n",
    "model, scaler, numeric_cols, non_numeric_cols = train_xgboost(\n",
    "   '/Users/rishav/Downloads/modified_data.csv',\n",
    "    target_column='WeightedExpense'\n",
    ")\n",
    "\n",
    "# Predict on Test CSV\n",
    "test_results = predict_on_test_data(\n",
    "    test_csv_path='/Users/rishav/Downloads/test_data_final.csv',\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    numeric_cols=numeric_cols,\n",
    "    non_numeric_cols=non_numeric_cols\n",
    ")\n",
    "\n",
    "# Aggregate predictions by household\n",
    "final_results = aggregate_household_predictions(test_results)\n",
    "\n",
    "# Check if test data has actual values\n",
    "if 'TotalExpense' in pd.read_csv('/Users/rishav/Downloads/final_test_dataset.csv').columns:\n",
    "    # Run comparison\n",
    "    metrics = compare_expenses(\n",
    "        predictions_csv='test_data_with_household_sums_xgb_direct.csv',\n",
    "        actual_csv='/Users/rishav/Downloads/final_test_dataset.csv',\n",
    "        pred_col='predicted_expense',\n",
    "        actual_col='TotalExpense',\n",
    "        join_col='HH_ID'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal evaluation metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training data shape: (901723, 50)\n",
      "Testing data shape: (225316, 50)\n",
      "\n",
      "Null value counts in test data:\n",
      "Series([], dtype: int64)\n",
      "No null values found in test data\n",
      "\n",
      "Training cluster-wise random_forest models...\n",
      "Optimal number of clusters (Elbow method): 7\n",
      "Training random_forest model for cluster 0 with 170811 samples\n",
      "\n",
      "Top 5 features for cluster 0:\n",
      "                    Feature  Importance\n",
      "0            Person Srl No.    0.132419\n",
      "22        HH Size (For FDQ)    0.129969\n",
      "1   Relation to head (code)    0.100957\n",
      "24                   NIC_5D    0.061566\n",
      "3             Age(in years)    0.053910\n",
      "Training random_forest model for cluster 1 with 221499 samples\n",
      "\n",
      "Top 5 features for cluster 1:\n",
      "              Feature  Importance\n",
      "18           District    0.107002\n",
      "17         NSS-Region    0.092360\n",
      "22  HH Size (For FDQ)    0.089114\n",
      "24             NIC_5D    0.083437\n",
      "3       Age(in years)    0.069685\n",
      "Training random_forest model for cluster 2 with 119558 samples\n",
      "\n",
      "Top 5 features for cluster 2:\n",
      "                    Feature  Importance\n",
      "22        HH Size (For FDQ)    0.126328\n",
      "3             Age(in years)    0.092126\n",
      "1   Relation to head (code)    0.085538\n",
      "0            Person Srl No.    0.083067\n",
      "18                 District    0.074336\n",
      "Training random_forest model for cluster 3 with 163831 samples\n",
      "\n",
      "Top 5 features for cluster 3:\n",
      "                    Feature  Importance\n",
      "18                 District    0.107087\n",
      "3             Age(in years)    0.104129\n",
      "17               NSS-Region    0.089408\n",
      "22        HH Size (For FDQ)    0.085690\n",
      "1   Relation to head (code)    0.068747\n",
      "Training random_forest model for cluster 4 with 87550 samples\n",
      "\n",
      "Top 5 features for cluster 4:\n",
      "                    Feature  Importance\n",
      "22        HH Size (For FDQ)    0.139393\n",
      "1   Relation to head (code)    0.074046\n",
      "0            Person Srl No.    0.070954\n",
      "3             Age(in years)    0.070589\n",
      "24                   NIC_5D    0.068641\n",
      "Training random_forest model for cluster 5 with 14874 samples\n",
      "\n",
      "Top 5 features for cluster 5:\n",
      "                         Feature  Importance\n",
      "22             HH Size (For FDQ)    0.130342\n",
      "42  Is_HH_Have_Motorcar_jeep_van    0.087196\n",
      "23                        NCO_3D    0.071835\n",
      "1        Relation to head (code)    0.064049\n",
      "3                  Age(in years)    0.054536\n",
      "Training random_forest model for cluster 6 with 123600 samples\n",
      "\n",
      "Top 5 features for cluster 6:\n",
      "              Feature  Importance\n",
      "22  HH Size (For FDQ)    0.090357\n",
      "3       Age(in years)    0.085026\n",
      "18           District    0.073536\n",
      "24             NIC_5D    0.065570\n",
      "17         NSS-Region    0.060186\n",
      "\n",
      "--- Cluster-wise Random_forest Prediction ---\n",
      "Mean Percentage Error (MPE): 8.61%\n",
      "Mean Absolute Error (MAE): 416.76\n",
      "R² Score: 0.9460\n",
      "\n",
      "--- Cluster Mean Assignment ---\n",
      "Mean Percentage Error (MPE): 46.71%\n",
      "Mean Absolute Error (MAE): 1922.44\n",
      "R² Score: 0.2312\n",
      "\n",
      "Making predictions on test data...\n",
      "All required numeric columns found in test data\n",
      "Made predictions for 42839 samples in cluster 0\n",
      "Made predictions for 54088 samples in cluster 1\n",
      "Made predictions for 30338 samples in cluster 2\n",
      "Made predictions for 41614 samples in cluster 3\n",
      "Made predictions for 21727 samples in cluster 4\n",
      "Made predictions for 3706 samples in cluster 5\n",
      "Made predictions for 31004 samples in cluster 6\n",
      "\n",
      "Sample Predictions:\n",
      "   predicted_expense  assigned_cluster\n",
      "0        6152.236371                 3\n",
      "1        4721.460102                 2\n",
      "2        4852.511551                 2\n",
      "3        3509.550519                 2\n",
      "4       10113.204024                 4\n",
      "\n",
      "Aggregating predictions by household...\n",
      "\n",
      "Sample rows from household HCES2022655561010131113011101202304:\n",
      "                                 HH_ID  predicted_expense  assigned_cluster\n",
      "0  HCES2022655561010131113011101202304       19235.758544                 3\n",
      "1  HCES2022655561010131113011101202304       19235.758544                 2\n",
      "2  HCES2022655561010131113011101202304       19235.758544                 2\n",
      "3  HCES2022655561010131113011101202304       19235.758544                 2\n",
      "\n",
      "Final results saved to test_predictions_random_forest.csv\n",
      "\n",
      "Actual values found in test data. Comparing predictions...\n",
      "Number of households for comparison: 52350\n",
      "\n",
      "--- Final Evaluation ---\n",
      "Mean Percentage Error (MPE): 28.20%\n",
      "Mean Absolute Error (MAE): 5369.92\n",
      "R² Score: 0.5376\n",
      "\n",
      "Detailed comparison saved to 'expense_comparison_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Standardize numeric features, exclude non-numeric (e.g., IDs)\n",
    "def standardize_features(df, target_column):\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    if target_column in non_numeric_cols:\n",
    "        non_numeric_cols.remove(target_column)\n",
    "\n",
    "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist()\n",
    "\n",
    "# Elbow method to find optimal clusters\n",
    "def elbow_method_auto(X, max_clusters=10):\n",
    "    inertias = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    # Find elbow: largest drop in inertia\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deltas = np.diff(deltas)\n",
    "    elbow_point = np.argmin(second_deltas) + 2  # +2 accounts for double diff index shift\n",
    "    return elbow_point\n",
    "\n",
    "# Mean Percentage Error\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    return np.nan\n",
    "\n",
    "# Evaluate and print metrics\n",
    "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
    "    mpe = mean_percentage_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {method_name} ---\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mpe': mpe,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Select model based on model_type\n",
    "def get_model(model_type):\n",
    "    if model_type == 'linear':\n",
    "        return LinearRegression()\n",
    "    elif model_type == 'random_forest':\n",
    "        return RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    elif model_type == 'xgboost':\n",
    "        return XGBRegressor(random_state=42, n_estimators=100, verbosity=0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'linear', 'random_forest', or 'xgboost'.\")\n",
    "\n",
    "# Main function: cluster, predict, evaluate\n",
    "def cluster_and_predict(train_df, target_column='WeightedExpense', max_clusters=10, model_type='random_forest'):\n",
    "    \"\"\"\n",
    "    Implement clustering approach with selected model for each cluster\n",
    "    \"\"\"\n",
    "    # Step 1: Standardize numeric features\n",
    "    X_scaled, y, scaler, non_numeric_cols, numeric_cols = standardize_features(train_df, target_column)\n",
    "\n",
    "    # Step 2: Find optimal clusters\n",
    "    optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
    "    print(f\"Optimal number of clusters (Elbow method): {optimal_k}\")\n",
    "\n",
    "    # Step 3: KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Add cluster labels to DataFrame for reference\n",
    "    df_with_clusters = train_df.copy()\n",
    "    df_with_clusters['Cluster'] = clusters\n",
    "\n",
    "    # Step 4A: Cluster-wise model training\n",
    "    preds_model = np.zeros_like(y)\n",
    "    cluster_models = {}\n",
    "    \n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        X_cluster = X_scaled[cluster_idx]\n",
    "        y_cluster = y[cluster_idx]\n",
    "        \n",
    "        print(f\"Training {model_type} model for cluster {cluster_id} with {len(y_cluster)} samples\")\n",
    "        \n",
    "        # Skip empty clusters (shouldn't happen but just in case)\n",
    "        if len(y_cluster) == 0:\n",
    "            continue\n",
    "\n",
    "        # Train model for this cluster\n",
    "        model = get_model(model_type)\n",
    "        model.fit(X_cluster, y_cluster)\n",
    "        \n",
    "        # Make predictions for this cluster\n",
    "        preds_model[cluster_idx] = model.predict(X_cluster)\n",
    "\n",
    "        # Save model per cluster\n",
    "        cluster_models[cluster_id] = model\n",
    "        \n",
    "        # Print feature importance if available\n",
    "        if hasattr(model, 'feature_importances_') and len(numeric_cols) > 0:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': numeric_cols,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 5 features for cluster {cluster_id}:\")\n",
    "            print(feature_importance.head(5))\n",
    "\n",
    "    # Evaluate cluster-wise model\n",
    "    evaluate_predictions(y, preds_model, method_name=f\"Cluster-wise {model_type.capitalize()} Prediction\")\n",
    "\n",
    "    # Step 4B: Cluster mean assignment (as a simple baseline)\n",
    "    preds_mean = np.zeros_like(y)\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        y_cluster = y[cluster_idx]\n",
    "        cluster_mean = np.mean(y_cluster)\n",
    "        preds_mean[cluster_idx] = cluster_mean\n",
    "\n",
    "    # Evaluate cluster mean assignment\n",
    "    evaluate_predictions(y, preds_mean, method_name=\"Cluster Mean Assignment\")\n",
    "\n",
    "    return preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k\n",
    "\n",
    "# FIXED: Predict on test data using cluster-specific models\n",
    "def predict_on_test_data(test_df, kmeans, cluster_models, scaler, numeric_cols, non_numeric_cols):\n",
    "    \"\"\"\n",
    "    Predict expenses on test data using trained cluster-specific models\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with only the numeric columns in the correct order\n",
    "    X_numeric_test = pd.DataFrame(index=test_df.index)\n",
    "    \n",
    "    # Track any columns missing from test data\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in test_df.columns:\n",
    "            # Check if this specific column has nulls before adding it\n",
    "            if test_df[col].isnull().any():\n",
    "                # Fill nulls in this column with its median before adding\n",
    "                X_numeric_test[col] = test_df[col].fillna(test_df[col].median())\n",
    "                print(f\"Filled nulls in column '{col}' with its median\")\n",
    "            else:\n",
    "                X_numeric_test[col] = test_df[col]\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "            print(f\"Warning: Column '{col}' missing from test data. Using zeros.\")\n",
    "            X_numeric_test[col] = 0\n",
    "    \n",
    "    # Report on missing columns if any\n",
    "    if missing_columns:\n",
    "        print(f\"\\nTotal missing columns: {len(missing_columns)}\")\n",
    "        if len(missing_columns) <= 10:  # Only print if there aren't too many\n",
    "            print(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "        else:\n",
    "            print(f\"First 10 missing columns: {', '.join(missing_columns[:10])}...\")\n",
    "    else:\n",
    "        print(\"All required numeric columns found in test data\")\n",
    "    \n",
    "    # Standardize features using the same scaler from training\n",
    "    X_scaled_test = scaler.transform(X_numeric_test)\n",
    "    \n",
    "    # Predict clusters for test data\n",
    "    test_clusters = kmeans.predict(X_scaled_test)\n",
    "    \n",
    "    # Predict using cluster-specific models\n",
    "    predictions = np.zeros(X_scaled_test.shape[0])\n",
    "    \n",
    "    for cluster_id in np.unique(test_clusters):\n",
    "        cluster_idx = test_clusters == cluster_id\n",
    "        X_cluster_test = X_scaled_test[cluster_idx]\n",
    "        \n",
    "        if cluster_id in cluster_models:\n",
    "            model = cluster_models[cluster_id]\n",
    "            predictions[cluster_idx] = model.predict(X_cluster_test)\n",
    "            print(f\"Made predictions for {np.sum(cluster_idx)} samples in cluster {cluster_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: No model for cluster {cluster_id}, using average prediction\")\n",
    "            # Use average of all predictions for this cluster\n",
    "            cluster_means = np.mean([m.predict(X_cluster_test).mean() \n",
    "                                     for m in cluster_models.values()])\n",
    "            predictions[cluster_idx] = cluster_means\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    test_df_with_preds = test_df.copy()\n",
    "    test_df_with_preds['predicted_expense'] = predictions\n",
    "    test_df_with_preds['assigned_cluster'] = test_clusters\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(test_df_with_preds[['predicted_expense', 'assigned_cluster']].head())\n",
    "    \n",
    "    return test_df_with_preds\n",
    "\n",
    "# Aggregate by household ID\n",
    "def aggregate_household_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by household ID\n",
    "    \"\"\"\n",
    "    if 'HH_ID' not in test_results.columns:\n",
    "        print(\"Warning: 'HH_ID' column not found. Cannot aggregate by household.\")\n",
    "        return test_results\n",
    "    \n",
    "    # Calculate the sum of predicted_expense for each household ID\n",
    "    household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
    "    \n",
    "    # Replace each row's predicted_expense with its household sum\n",
    "    test_results_aggregated = test_results.copy()\n",
    "    test_results_aggregated['predicted_expense'] = test_results_aggregated['HH_ID'].map(household_sums)\n",
    "    \n",
    "    # Verify the result with a sample household\n",
    "    sample_household = test_results_aggregated['HH_ID'].iloc[0]\n",
    "    sample_rows = test_results_aggregated[test_results_aggregated['HH_ID'] == sample_household].head()\n",
    "    print(f\"\\nSample rows from household {sample_household}:\")\n",
    "    print(sample_rows[['HH_ID', 'predicted_expense', 'assigned_cluster']])\n",
    "    \n",
    "    return test_results_aggregated\n",
    "\n",
    "# Compare predictions with actual values\n",
    "def compare_expenses(predictions_df, actual_df, \n",
    "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
    "                     join_col='HH_ID'):\n",
    "    \"\"\"\n",
    "    Compare predicted expenses with actual expenses\n",
    "    \"\"\"\n",
    "    # Check if the datasets have the required columns\n",
    "    required_cols = {\n",
    "        'predictions': [pred_col, join_col],\n",
    "        'actual': [actual_col, join_col]\n",
    "    }\n",
    "    \n",
    "    for df_name, cols in required_cols.items():\n",
    "        df = predictions_df if df_name == 'predictions' else actual_df\n",
    "        missing = [col for col in cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
    "    \n",
    "    # If we have multiple rows per household, keep only one row per household in each dataset\n",
    "    pred_unique = predictions_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
    "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
    "    \n",
    "    # Print the number of households we can compare\n",
    "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
    "    \n",
    "    # Extract the values for comparison\n",
    "    y_pred = merged_df[pred_col].values\n",
    "    y_true = merged_df[actual_col].values\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    metrics = evaluate_predictions(y_true, y_pred, method_name=\"Final Evaluation\")\n",
    "    \n",
    "    # Save the comparison to a new CSV for further analysis\n",
    "    merged_df['error'] = y_pred - y_true\n",
    "    merged_df['percentage_error'] = (merged_df['error'] / merged_df[actual_col]) * 100\n",
    "    merged_df.to_csv('expense_comparison_results.csv', index=False)\n",
    "    print(\"\\nDetailed comparison saved to 'expense_comparison_results.csv'\")\n",
    "    \n",
    "    return metrics, merged_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load datasets with paths from the provided code\n",
    "    print(\"Loading datasets...\")\n",
    "    train_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv'  # Using the path from the provided code\n",
    "    test_path = '/Users/rishav/Downloads/test_data_final.csv'  # Using the path from the provided code\n",
    "    \n",
    "    try:\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Testing data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Print information about null values in test data\n",
    "        null_counts = test_data.isnull().sum()\n",
    "        print(\"\\nNull value counts in test data:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        if null_counts.sum() == 0:\n",
    "            print(\"No null values found in test data\")\n",
    "        \n",
    "        # 2. Fixed parameters\n",
    "        target_column = 'WeightedExpense'\n",
    "        model_type = 'random_forest'  # Could be 'linear', 'random_forest', or 'xgboost'\n",
    "        \n",
    "        # 3. Train cluster-wise models\n",
    "        print(f\"\\nTraining cluster-wise {model_type} models...\")\n",
    "        preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k = cluster_and_predict(\n",
    "            train_data, \n",
    "            target_column=target_column,\n",
    "            max_clusters=10,\n",
    "            model_type=model_type\n",
    "        )\n",
    "\n",
    "        # 4. Make predictions on test data\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_with_predictions = predict_on_test_data(\n",
    "            test_data,\n",
    "            kmeans,\n",
    "            cluster_models,\n",
    "            scaler,\n",
    "            numeric_cols,\n",
    "            non_numeric_cols\n",
    "        )\n",
    "\n",
    "        # 5. Aggregate predictions by household ID\n",
    "        print(\"\\nAggregating predictions by household...\")\n",
    "        final_results = aggregate_household_predictions(test_with_predictions)\n",
    "\n",
    "        # 6. Save the final results\n",
    "        output_path = f'test_predictions_{model_type}.csv'\n",
    "        final_results.to_csv(output_path, index=False)\n",
    "        print(f\"\\nFinal results saved to {output_path}\")\n",
    "\n",
    "        # 7. Compare with actual values (since we're loading final_test_dataset.csv which should have actual values)\n",
    "        if 'TotalExpense' in test_data.columns:\n",
    "            print(\"\\nActual values found in test data. Comparing predictions...\")\n",
    "            metrics, comparison_df = compare_expenses(\n",
    "                final_results,\n",
    "                test_data,\n",
    "                pred_col='predicted_expense',\n",
    "                actual_col='TotalExpense',\n",
    "                join_col='HH_ID'\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nNo 'TotalExpense' column found in test data. Skipping comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training data shape: (901723, 50)\n",
      "Testing data shape: (225316, 50)\n",
      "\n",
      "Null value counts in test data:\n",
      "Series([], dtype: int64)\n",
      "No null values found in test data\n",
      "\n",
      "Training cluster-wise xgboost models...\n",
      "Optimal number of clusters (Elbow method): 7\n",
      "Training xgboost model for cluster 0 with 170811 samples\n",
      "\n",
      "Top 5 features for cluster 0:\n",
      "                       Feature  Importance\n",
      "0               Person Srl No.    0.240340\n",
      "22           HH Size (For FDQ)    0.080720\n",
      "45     Is_HH_Have_Refrigerator    0.059460\n",
      "1      Relation to head (code)    0.056594\n",
      "46  Is_HH_Have_Washing_machine    0.048659\n",
      "Training xgboost model for cluster 1 with 221499 samples\n",
      "\n",
      "Top 5 features for cluster 1:\n",
      "                          Feature  Importance\n",
      "45        Is_HH_Have_Refrigerator    0.162994\n",
      "22              HH Size (For FDQ)    0.052001\n",
      "42   Is_HH_Have_Motorcar_jeep_van    0.051991\n",
      "36          Is_HH_Have_Television    0.045130\n",
      "41  Is_HH_Have_Motorcycle_scooter    0.045014\n",
      "Training xgboost model for cluster 2 with 119558 samples\n",
      "\n",
      "Top 5 features for cluster 2:\n",
      "                                              Feature  Importance\n",
      "0                                      Person Srl No.    0.191149\n",
      "22                                  HH Size (For FDQ)    0.065539\n",
      "42                       Is_HH_Have_Motorcar_jeep_van    0.060616\n",
      "46                         Is_HH_Have_Washing_machine    0.045552\n",
      "11  No. of meals taken during last 30 days from em...    0.033243\n",
      "Training xgboost model for cluster 3 with 163831 samples\n",
      "\n",
      "Top 5 features for cluster 3:\n",
      "                                              Feature  Importance\n",
      "0                                      Person Srl No.    0.184640\n",
      "22                                  HH Size (For FDQ)    0.050548\n",
      "7   Whether used internet from any location during...    0.048634\n",
      "25               Is_online_Clothing_Purchased_Last365    0.042322\n",
      "5           Highest educational level attained (code)    0.040728\n",
      "Training xgboost model for cluster 4 with 87550 samples\n",
      "\n",
      "Top 5 features for cluster 4:\n",
      "                                Feature  Importance\n",
      "0                        Person Srl No.    0.166646\n",
      "42         Is_HH_Have_Motorcar_jeep_van    0.078829\n",
      "22                    HH Size (For FDQ)    0.071321\n",
      "38                 Is_HH_Have_Laptop_PC    0.039813\n",
      "47  Is_HH_Have_Airconditioner_aircooler    0.032921\n",
      "Training xgboost model for cluster 5 with 14874 samples\n",
      "\n",
      "Top 5 features for cluster 5:\n",
      "                                              Feature  Importance\n",
      "46                         Is_HH_Have_Washing_machine    0.139834\n",
      "42                       Is_HH_Have_Motorcar_jeep_van    0.119322\n",
      "38                               Is_HH_Have_Laptop_PC    0.069219\n",
      "22                                  HH Size (For FDQ)    0.055239\n",
      "8   No. of days stayed away from home during last ...    0.053595\n",
      "Training xgboost model for cluster 6 with 123600 samples\n",
      "\n",
      "Top 5 features for cluster 6:\n",
      "                       Feature  Importance\n",
      "45     Is_HH_Have_Refrigerator    0.150747\n",
      "0               Person Srl No.    0.100554\n",
      "46  Is_HH_Have_Washing_machine    0.075944\n",
      "22           HH Size (For FDQ)    0.051849\n",
      "1      Relation to head (code)    0.044064\n",
      "\n",
      "--- Cluster-wise Xgboost Prediction ---\n",
      "Mean Percentage Error (MPE): 24.17%\n",
      "Mean Absolute Error (MAE): 1081.33\n",
      "R² Score: 0.7501\n",
      "\n",
      "--- Cluster Mean Assignment ---\n",
      "Mean Percentage Error (MPE): 46.71%\n",
      "Mean Absolute Error (MAE): 1922.44\n",
      "R² Score: 0.2312\n",
      "\n",
      "Making predictions on test data...\n",
      "All required numeric columns found in test data\n",
      "Made predictions for 42075 samples in cluster 0\n",
      "Made predictions for 55738 samples in cluster 1\n",
      "Made predictions for 29688 samples in cluster 2\n",
      "Made predictions for 41565 samples in cluster 3\n",
      "Made predictions for 21690 samples in cluster 4\n",
      "Made predictions for 3707 samples in cluster 5\n",
      "Made predictions for 30853 samples in cluster 6\n",
      "\n",
      "Sample Predictions:\n",
      "   predicted_expense  assigned_cluster\n",
      "0        6413.788086                 3\n",
      "1        3751.802246                 2\n",
      "2        3435.598145                 2\n",
      "3        3247.434082                 2\n",
      "4        7632.594238                 4\n",
      "\n",
      "Aggregating predictions by household...\n",
      "\n",
      "Sample rows from household HCES2022655561010131113011101202304:\n",
      "                                 HH_ID  predicted_expense  assigned_cluster\n",
      "0  HCES2022655561010131113011101202304       16848.622559                 3\n",
      "1  HCES2022655561010131113011101202304       16848.622559                 2\n",
      "2  HCES2022655561010131113011101202304       16848.622559                 2\n",
      "3  HCES2022655561010131113011101202304       16848.622559                 2\n",
      "\n",
      "Final results saved to test_predictions_xgboost.csv\n",
      "\n",
      "Actual values found in test data. Comparing predictions...\n",
      "Number of households for comparison: 52350\n",
      "\n",
      "--- Final Evaluation ---\n",
      "Mean Percentage Error (MPE): 26.70%\n",
      "Mean Absolute Error (MAE): 5143.34\n",
      "R² Score: 0.5606\n",
      "\n",
      "Detailed comparison saved to 'expense_comparison_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Standardize numeric features, exclude non-numeric (e.g., IDs)\n",
    "def standardize_features(df, target_column):\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    if target_column in non_numeric_cols:\n",
    "        non_numeric_cols.remove(target_column)\n",
    "\n",
    "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist()\n",
    "\n",
    "# Elbow method to find optimal clusters\n",
    "def elbow_method_auto(X, max_clusters=10):\n",
    "    inertias = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    # Find elbow: largest drop in inertia\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deltas = np.diff(deltas)\n",
    "    elbow_point = np.argmin(second_deltas) + 2  # +2 accounts for double diff index shift\n",
    "    return elbow_point\n",
    "\n",
    "# Mean Percentage Error\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    return np.nan\n",
    "\n",
    "# Evaluate and print metrics\n",
    "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
    "    mpe = mean_percentage_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {method_name} ---\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mpe': mpe,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Select model based on model_type\n",
    "def get_model(model_type):\n",
    "    if model_type == 'linear':\n",
    "        return LinearRegression()\n",
    "    elif model_type == 'random_forest':\n",
    "        return RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    elif model_type == 'xgboost':\n",
    "        return XGBRegressor(random_state=42, n_estimators=100, verbosity=0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'linear', 'random_forest', or 'xgboost'.\")\n",
    "\n",
    "# Main function: cluster, predict, evaluate\n",
    "def cluster_and_predict(train_df, target_column='WeightedExpense', max_clusters=10, model_type='random_forest'):\n",
    "    \"\"\"\n",
    "    Implement clustering approach with selected model for each cluster\n",
    "    \"\"\"\n",
    "    # Step 1: Standardize numeric features\n",
    "    X_scaled, y, scaler, non_numeric_cols, numeric_cols = standardize_features(train_df, target_column)\n",
    "\n",
    "    # Step 2: Find optimal clusters\n",
    "    optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
    "    print(f\"Optimal number of clusters (Elbow method): {optimal_k}\")\n",
    "\n",
    "    # Step 3: KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Add cluster labels to DataFrame for reference\n",
    "    df_with_clusters = train_df.copy()\n",
    "    df_with_clusters['Cluster'] = clusters\n",
    "\n",
    "    # Step 4A: Cluster-wise model training\n",
    "    preds_model = np.zeros_like(y)\n",
    "    cluster_models = {}\n",
    "    \n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        X_cluster = X_scaled[cluster_idx]\n",
    "        y_cluster = y[cluster_idx]\n",
    "        \n",
    "        print(f\"Training {model_type} model for cluster {cluster_id} with {len(y_cluster)} samples\")\n",
    "        \n",
    "        # Skip empty clusters (shouldn't happen but just in case)\n",
    "        if len(y_cluster) == 0:\n",
    "            continue\n",
    "\n",
    "        # Train model for this cluster\n",
    "        model = get_model(model_type)\n",
    "        model.fit(X_cluster, y_cluster)\n",
    "        \n",
    "        # Make predictions for this cluster\n",
    "        preds_model[cluster_idx] = model.predict(X_cluster)\n",
    "\n",
    "        # Save model per cluster\n",
    "        cluster_models[cluster_id] = model\n",
    "        \n",
    "        # Print feature importance if available\n",
    "        if hasattr(model, 'feature_importances_') and len(numeric_cols) > 0:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': numeric_cols,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 5 features for cluster {cluster_id}:\")\n",
    "            print(feature_importance.head(5))\n",
    "\n",
    "    # Evaluate cluster-wise model\n",
    "    evaluate_predictions(y, preds_model, method_name=f\"Cluster-wise {model_type.capitalize()} Prediction\")\n",
    "\n",
    "    # Step 4B: Cluster mean assignment (as a simple baseline)\n",
    "    preds_mean = np.zeros_like(y)\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        y_cluster = y[cluster_idx]\n",
    "        cluster_mean = np.mean(y_cluster)\n",
    "        preds_mean[cluster_idx] = cluster_mean\n",
    "\n",
    "    # Evaluate cluster mean assignment\n",
    "    evaluate_predictions(y, preds_mean, method_name=\"Cluster Mean Assignment\")\n",
    "\n",
    "    return preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k\n",
    "\n",
    "# FIXED: Predict on test data using cluster-specific models\n",
    "def predict_on_test_data(test_df, kmeans, cluster_models, scaler, numeric_cols, non_numeric_cols):\n",
    "    \"\"\"\n",
    "    Predict expenses on test data using trained cluster-specific models\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with only the numeric columns in the correct order\n",
    "    X_numeric_test = pd.DataFrame(index=test_df.index)\n",
    "    \n",
    "    # Track any columns missing from test data\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in test_df.columns:\n",
    "            # Check if this specific column has nulls before adding it\n",
    "            if test_df[col].isnull().any():\n",
    "                # Fill nulls in this column with its median before adding\n",
    "                X_numeric_test[col] = test_df[col].fillna(test_df[col].median())\n",
    "                print(f\"Filled nulls in column '{col}' with its median\")\n",
    "            else:\n",
    "                X_numeric_test[col] = test_df[col]\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "            print(f\"Warning: Column '{col}' missing from test data. Using zeros.\")\n",
    "            X_numeric_test[col] = 0\n",
    "    \n",
    "    # Report on missing columns if any\n",
    "    if missing_columns:\n",
    "        print(f\"\\nTotal missing columns: {len(missing_columns)}\")\n",
    "        if len(missing_columns) <= 10:  # Only print if there aren't too many\n",
    "            print(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "        else:\n",
    "            print(f\"First 10 missing columns: {', '.join(missing_columns[:10])}...\")\n",
    "    else:\n",
    "        print(\"All required numeric columns found in test data\")\n",
    "    \n",
    "    # Standardize features using the same scaler from training\n",
    "    X_scaled_test = scaler.transform(X_numeric_test)\n",
    "    \n",
    "    # Predict clusters for test data\n",
    "    test_clusters = kmeans.predict(X_scaled_test)\n",
    "    \n",
    "    # Predict using cluster-specific models\n",
    "    predictions = np.zeros(X_scaled_test.shape[0])\n",
    "    \n",
    "    for cluster_id in np.unique(test_clusters):\n",
    "        cluster_idx = test_clusters == cluster_id\n",
    "        X_cluster_test = X_scaled_test[cluster_idx]\n",
    "        \n",
    "        if cluster_id in cluster_models:\n",
    "            model = cluster_models[cluster_id]\n",
    "            predictions[cluster_idx] = model.predict(X_cluster_test)\n",
    "            print(f\"Made predictions for {np.sum(cluster_idx)} samples in cluster {cluster_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: No model for cluster {cluster_id}, using average prediction\")\n",
    "            # Use average of all predictions for this cluster\n",
    "            cluster_means = np.mean([m.predict(X_cluster_test).mean() \n",
    "                                     for m in cluster_models.values()])\n",
    "            predictions[cluster_idx] = cluster_means\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    test_df_with_preds = test_df.copy()\n",
    "    test_df_with_preds['predicted_expense'] = predictions\n",
    "    test_df_with_preds['assigned_cluster'] = test_clusters\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(test_df_with_preds[['predicted_expense', 'assigned_cluster']].head())\n",
    "    \n",
    "    return test_df_with_preds\n",
    "\n",
    "# Aggregate by household ID\n",
    "def aggregate_household_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by household ID\n",
    "    \"\"\"\n",
    "    if 'HH_ID' not in test_results.columns:\n",
    "        print(\"Warning: 'HH_ID' column not found. Cannot aggregate by household.\")\n",
    "        return test_results\n",
    "    \n",
    "    # Calculate the sum of predicted_expense for each household ID\n",
    "    household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
    "    \n",
    "    # Replace each row's predicted_expense with its household sum\n",
    "    test_results_aggregated = test_results.copy()\n",
    "    test_results_aggregated['predicted_expense'] = test_results_aggregated['HH_ID'].map(household_sums)\n",
    "    \n",
    "    # Verify the result with a sample household\n",
    "    sample_household = test_results_aggregated['HH_ID'].iloc[0]\n",
    "    sample_rows = test_results_aggregated[test_results_aggregated['HH_ID'] == sample_household].head()\n",
    "    print(f\"\\nSample rows from household {sample_household}:\")\n",
    "    print(sample_rows[['HH_ID', 'predicted_expense', 'assigned_cluster']])\n",
    "    \n",
    "    return test_results_aggregated\n",
    "\n",
    "# Compare predictions with actual values\n",
    "def compare_expenses(predictions_df, actual_df, \n",
    "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
    "                     join_col='HH_ID'):\n",
    "    \"\"\"\n",
    "    Compare predicted expenses with actual expenses\n",
    "    \"\"\"\n",
    "    # Check if the datasets have the required columns\n",
    "    required_cols = {\n",
    "        'predictions': [pred_col, join_col],\n",
    "        'actual': [actual_col, join_col]\n",
    "    }\n",
    "    \n",
    "    for df_name, cols in required_cols.items():\n",
    "        df = predictions_df if df_name == 'predictions' else actual_df\n",
    "        missing = [col for col in cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
    "    \n",
    "    # If we have multiple rows per household, keep only one row per household in each dataset\n",
    "    pred_unique = predictions_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
    "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
    "    \n",
    "    # Print the number of households we can compare\n",
    "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
    "    \n",
    "    # Extract the values for comparison\n",
    "    y_pred = merged_df[pred_col].values\n",
    "    y_true = merged_df[actual_col].values\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    metrics = evaluate_predictions(y_true, y_pred, method_name=\"Final Evaluation\")\n",
    "    \n",
    "    # Save the comparison to a new CSV for further analysis\n",
    "    merged_df['error'] = y_pred - y_true\n",
    "    merged_df['percentage_error'] = (merged_df['error'] / merged_df[actual_col]) * 100\n",
    "    merged_df.to_csv('expense_comparison_results.csv', index=False)\n",
    "    print(\"\\nDetailed comparison saved to 'expense_comparison_results.csv'\")\n",
    "    \n",
    "    return metrics, merged_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load datasets with paths from the provided code\n",
    "    print(\"Loading datasets...\")\n",
    "    train_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv'  # Using the path from the provided code\n",
    "    test_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/test_data_final.csv'  # Using the path from the provided code\n",
    "    \n",
    "    try:\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Testing data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Print information about null values in test data\n",
    "        null_counts = test_data.isnull().sum()\n",
    "        print(\"\\nNull value counts in test data:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        if null_counts.sum() == 0:\n",
    "            print(\"No null values found in test data\")\n",
    "        \n",
    "        # 2. Fixed parameters\n",
    "        target_column = 'WeightedExpense'\n",
    "        model_type = 'xgboost'  # Changed from 'random_forest' to 'xgboost'\n",
    "        \n",
    "        # 3. Train cluster-wise models\n",
    "        print(f\"\\nTraining cluster-wise {model_type} models...\")\n",
    "        preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k = cluster_and_predict(\n",
    "            train_data, \n",
    "            target_column=target_column,\n",
    "            max_clusters=10,\n",
    "            model_type=model_type\n",
    "        )\n",
    "\n",
    "        # 4. Make predictions on test data\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_with_predictions = predict_on_test_data(\n",
    "            test_data,\n",
    "            kmeans,\n",
    "            cluster_models,\n",
    "            scaler,\n",
    "            numeric_cols,\n",
    "            non_numeric_cols\n",
    "        )\n",
    "\n",
    "        # 5. Aggregate predictions by household ID\n",
    "        print(\"\\nAggregating predictions by household...\")\n",
    "        final_results = aggregate_household_predictions(test_with_predictions)\n",
    "\n",
    "        # 6. Save the final results\n",
    "        output_path = f'test_predictions_{model_type}.csv'\n",
    "        final_results.to_csv(output_path, index=False)\n",
    "        print(f\"\\nFinal results saved to {output_path}\")\n",
    "\n",
    "        # 7. Compare with actual values (since we're loading final_test_dataset.csv which should have actual values)\n",
    "        if 'TotalExpense' in test_data.columns:\n",
    "            print(\"\\nActual values found in test data. Comparing predictions...\")\n",
    "            metrics, comparison_df = compare_expenses(\n",
    "                final_results,\n",
    "                test_data,\n",
    "                pred_col='predicted_expense',\n",
    "                actual_col='TotalExpense',\n",
    "                join_col='HH_ID'\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nNo 'TotalExpense' column found in test data. Skipping comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training data shape: (901723, 50)\n",
      "Testing data shape: (225316, 50)\n",
      "\n",
      "Null value counts in test data:\n",
      "Series([], dtype: int64)\n",
      "No null values found in test data\n",
      "\n",
      "Training cluster-wise xgboost models...\n",
      "Using manual override for number of clusters: 5\n",
      "Using 5 clusters\n",
      "Training xgboost model for cluster 0 with 123169 samples\n",
      "Reduced features from 48 to 15\n",
      "XGBoost model fitted successfully for cluster\n",
      "\n",
      "Top 5 features for cluster 0:\n",
      "                         Feature  Importance\n",
      "0                 Person Srl No.    0.405651\n",
      "12  Is_HH_Have_Motorcar_jeep_van    0.124966\n",
      "8              HH Size (For FDQ)    0.124617\n",
      "10          Is_HH_Have_Laptop_PC    0.060803\n",
      "13    Is_HH_Have_Washing_machine    0.041186\n",
      "Training xgboost model for cluster 1 with 259209 samples\n",
      "Reduced features from 48 to 15\n",
      "XGBoost model fitted successfully for cluster\n",
      "\n",
      "Top 5 features for cluster 1:\n",
      "                                 Feature  Importance\n",
      "13               Is_HH_Have_Refrigerator    0.319152\n",
      "12          Is_HH_Have_Motorcar_jeep_van    0.078907\n",
      "11         Is_HH_Have_Motorcycle_scooter    0.065427\n",
      "6                      HH Size (For FDQ)    0.065167\n",
      "7   Is_online_Clothing_Purchased_Last365    0.057930\n",
      "Training xgboost model for cluster 2 with 242752 samples\n",
      "Reduced features from 48 to 15\n",
      "XGBoost model fitted successfully for cluster\n",
      "\n",
      "Top 5 features for cluster 2:\n",
      "                       Feature  Importance\n",
      "0               Person Srl No.    0.445560\n",
      "7            HH Size (For FDQ)    0.100202\n",
      "13     Is_HH_Have_Refrigerator    0.076479\n",
      "14  Is_HH_Have_Washing_machine    0.075229\n",
      "1      Relation to head (code)    0.055996\n",
      "Training xgboost model for cluster 3 with 260671 samples\n",
      "Reduced features from 48 to 15\n",
      "XGBoost model fitted successfully for cluster\n",
      "\n",
      "Top 5 features for cluster 3:\n",
      "                          Feature  Importance\n",
      "0                  Person Srl No.    0.198667\n",
      "13        Is_HH_Have_Refrigerator    0.175902\n",
      "1         Relation to head (code)    0.090264\n",
      "7               HH Size (For FDQ)    0.073267\n",
      "11  Is_HH_Have_Motorcycle_scooter    0.060395\n",
      "Training xgboost model for cluster 4 with 15922 samples\n",
      "Reduced features from 48 to 15\n",
      "XGBoost model fitted successfully for cluster\n",
      "\n",
      "Top 5 features for cluster 4:\n",
      "                         Feature  Importance\n",
      "10  Is_HH_Have_Motorcar_jeep_van    0.181225\n",
      "0                 Person Srl No.    0.150441\n",
      "5              HH Size (For FDQ)    0.092593\n",
      "8           Is_HH_Have_Laptop_PC    0.090068\n",
      "13    Is_HH_Have_Washing_machine    0.088014\n",
      "\n",
      "--- Cluster-wise Xgboost Prediction ---\n",
      "Mean Percentage Error (MPE): 27.86%\n",
      "Mean Absolute Error (MAE): 1266.85\n",
      "R² Score: 0.6156\n",
      "\n",
      "--- Cluster Mean Assignment ---\n",
      "Mean Percentage Error (MPE): 47.24%\n",
      "Mean Absolute Error (MAE): 1937.55\n",
      "R² Score: 0.2217\n",
      "\n",
      "Making predictions on test data...\n",
      "All required numeric columns found in test data\n",
      "Made predictions for 30459 samples in cluster 0\n",
      "Made predictions for 65108 samples in cluster 1\n",
      "Made predictions for 60259 samples in cluster 2\n",
      "Made predictions for 65628 samples in cluster 3\n",
      "Made predictions for 3862 samples in cluster 4\n",
      "\n",
      "Sample Predictions:\n",
      "   predicted_expense  assigned_cluster\n",
      "0        7067.680664                 3\n",
      "1        4580.458008                 3\n",
      "2        4535.630371                 3\n",
      "3        3515.213135                 1\n",
      "4        9556.899414                 0\n",
      "\n",
      "Aggregating predictions by household...\n",
      "\n",
      "Sample rows from household HCES2022655561010131113011101202304:\n",
      "                                 HH_ID  predicted_expense  assigned_cluster\n",
      "0  HCES2022655561010131113011101202304       19698.982178                 3\n",
      "1  HCES2022655561010131113011101202304       19698.982178                 3\n",
      "2  HCES2022655561010131113011101202304       19698.982178                 3\n",
      "3  HCES2022655561010131113011101202304       19698.982178                 1\n",
      "\n",
      "Final results saved to test_predictions_xgboost_regularized.csv\n",
      "\n",
      "Actual values found in test data. Comparing predictions...\n",
      "Number of households for comparison: 52350\n",
      "\n",
      "--- Final Evaluation ---\n",
      "Mean Percentage Error (MPE): 28.39%\n",
      "Mean Absolute Error (MAE): 5414.08\n",
      "R² Score: 0.5308\n",
      "\n",
      "Detailed comparison saved to 'expense_comparison_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Standardize numeric features, exclude non-numeric (e.g., IDs)\n",
    "def standardize_features(df, target_column):\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    if target_column in non_numeric_cols:\n",
    "        non_numeric_cols.remove(target_column)\n",
    "\n",
    "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist()\n",
    "\n",
    "# Elbow method to find optimal clusters but with a manual override option\n",
    "def elbow_method_auto(X, max_clusters=10, manual_override=None):\n",
    "    if manual_override is not None:\n",
    "        print(f\"Using manual override for number of clusters: {manual_override}\")\n",
    "        return manual_override\n",
    "        \n",
    "    inertias = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    # Find elbow: largest drop in inertia\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deltas = np.diff(deltas)\n",
    "    elbow_point = np.argmin(second_deltas) + 2  # +2 accounts for double diff index shift\n",
    "    return elbow_point\n",
    "\n",
    "# Mean Percentage Error\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    return np.nan\n",
    "\n",
    "# Evaluate and print metrics\n",
    "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
    "    mpe = mean_percentage_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {method_name} ---\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mpe': mpe,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Select model based on model_type with regularization for XGBoost\n",
    "def get_model(model_type):\n",
    "    if model_type == 'linear':\n",
    "        return LinearRegression()\n",
    "    elif model_type == 'random_forest':\n",
    "        return RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    elif model_type == 'xgboost':\n",
    "        # Added regularization parameters to reduce overfitting\n",
    "        return XGBRegressor(\n",
    "            random_state=42, \n",
    "            n_estimators=100, \n",
    "            max_depth=5,           # Reduced from default 6\n",
    "            min_child_weight=3,    # Increased from default 1\n",
    "            reg_alpha=1.0,         # L1 regularization\n",
    "            reg_lambda=2.0,        # L2 regularization\n",
    "            verbosity=0\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Choose from 'linear', 'random_forest', or 'xgboost'.\")\n",
    "\n",
    "# Feature selection to keep only top important features\n",
    "def select_top_features(X, y, model_type='xgboost', top_n=15):\n",
    "    model = get_model(model_type)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(\"Model doesn't have feature importances, skipping feature selection\")\n",
    "        return X, list(range(X.shape[1]))\n",
    "    \n",
    "    # Create a feature selector using SelectFromModel\n",
    "    selector = SelectFromModel(model, threshold=-np.inf, max_features=top_n, prefit=True)\n",
    "    X_selected = selector.transform(X)\n",
    "    \n",
    "    # Get indices of selected features\n",
    "    selected_indices = np.where(selector.get_support())[0]\n",
    "    \n",
    "    print(f\"Reduced features from {X.shape[1]} to {X_selected.shape[1]}\")\n",
    "    return X_selected, selected_indices\n",
    "\n",
    "# Main function: cluster, predict, evaluate\n",
    "def cluster_and_predict(train_df, target_column='WeightedExpense', max_clusters=10, model_type='random_forest', manual_clusters=5):\n",
    "    \"\"\"\n",
    "    Implement clustering approach with selected model for each cluster\n",
    "    \"\"\"\n",
    "    # Step 1: Standardize numeric features\n",
    "    X_scaled, y, scaler, non_numeric_cols, numeric_cols = standardize_features(train_df, target_column)\n",
    "\n",
    "    # Step 2: Find optimal clusters (with manual override)\n",
    "    optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters, manual_override=manual_clusters)\n",
    "    print(f\"Using {optimal_k} clusters\")\n",
    "\n",
    "    # Step 3: KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Add cluster labels to DataFrame for reference\n",
    "    df_with_clusters = train_df.copy()\n",
    "    df_with_clusters['Cluster'] = clusters\n",
    "\n",
    "    # Step 4A: Cluster-wise model training\n",
    "    preds_model = np.zeros_like(y)\n",
    "    cluster_models = {}\n",
    "    feature_indices_per_cluster = {}\n",
    "    \n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        X_cluster = X_scaled[cluster_idx]\n",
    "        y_cluster = y[cluster_idx]\n",
    "        \n",
    "        print(f\"Training {model_type} model for cluster {cluster_id} with {len(y_cluster)} samples\")\n",
    "        \n",
    "        # Skip empty clusters (shouldn't happen but just in case)\n",
    "        if len(y_cluster) == 0:\n",
    "            continue\n",
    "\n",
    "        # Split data for early stopping\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cluster, y_cluster, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Feature selection - limit to top 15 most important features\n",
    "        X_train_selected, selected_indices = select_top_features(X_train, y_train, model_type, top_n=15)\n",
    "        X_val_selected = X_val[:, selected_indices]\n",
    "        \n",
    "        # Store selected feature indices for prediction\n",
    "        feature_indices_per_cluster[cluster_id] = selected_indices\n",
    "        \n",
    "        # Train model for this cluster\n",
    "        model = get_model(model_type)\n",
    "        \n",
    "        # Use early stopping if XGBoost - simplified approach without early stopping\n",
    "        if model_type == 'xgboost':\n",
    "            # Simply fit without early stopping since it's causing compatibility issues\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            print(f\"XGBoost model fitted successfully for cluster\")\n",
    "        else:\n",
    "            model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Make predictions for this cluster using selected features\n",
    "        X_cluster_selected = X_cluster[:, selected_indices]\n",
    "        preds_model[cluster_idx] = model.predict(X_cluster_selected)\n",
    "\n",
    "        # Save model per cluster\n",
    "        cluster_models[cluster_id] = model\n",
    "        \n",
    "        # Print feature importance if available\n",
    "        if hasattr(model, 'feature_importances_') and len(selected_indices) > 0:\n",
    "            selected_feature_names = [numeric_cols[i] for i in selected_indices]\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': selected_feature_names,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 5 features for cluster {cluster_id}:\")\n",
    "            print(feature_importance.head(5))\n",
    "\n",
    "    # Evaluate cluster-wise model\n",
    "    evaluate_predictions(y, preds_model, method_name=f\"Cluster-wise {model_type.capitalize()} Prediction\")\n",
    "\n",
    "    # Step 4B: Cluster mean assignment (as a simple baseline)\n",
    "    preds_mean = np.zeros_like(y)\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        y_cluster = y[cluster_idx]\n",
    "        cluster_mean = np.mean(y_cluster)\n",
    "        preds_mean[cluster_idx] = cluster_mean\n",
    "\n",
    "    # Evaluate cluster mean assignment\n",
    "    evaluate_predictions(y, preds_mean, method_name=\"Cluster Mean Assignment\")\n",
    "\n",
    "    return preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k, feature_indices_per_cluster\n",
    "\n",
    "# FIXED: Predict on test data using cluster-specific models\n",
    "def predict_on_test_data(test_df, kmeans, cluster_models, scaler, numeric_cols, non_numeric_cols, feature_indices_per_cluster):\n",
    "    \"\"\"\n",
    "    Predict expenses on test data using trained cluster-specific models\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with only the numeric columns in the correct order\n",
    "    X_numeric_test = pd.DataFrame(index=test_df.index)\n",
    "    \n",
    "    # Track any columns missing from test data\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in test_df.columns:\n",
    "            # Check if this specific column has nulls before adding it\n",
    "            if test_df[col].isnull().any():\n",
    "                # Fill nulls in this column with its median before adding\n",
    "                X_numeric_test[col] = test_df[col].fillna(test_df[col].median())\n",
    "                print(f\"Filled nulls in column '{col}' with its median\")\n",
    "            else:\n",
    "                X_numeric_test[col] = test_df[col]\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "            print(f\"Warning: Column '{col}' missing from test data. Using zeros.\")\n",
    "            X_numeric_test[col] = 0\n",
    "    \n",
    "    # Report on missing columns if any\n",
    "    if missing_columns:\n",
    "        print(f\"\\nTotal missing columns: {len(missing_columns)}\")\n",
    "        if len(missing_columns) <= 10:  # Only print if there aren't too many\n",
    "            print(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "        else:\n",
    "            print(f\"First 10 missing columns: {', '.join(missing_columns[:10])}...\")\n",
    "    else:\n",
    "        print(\"All required numeric columns found in test data\")\n",
    "    \n",
    "    # Standardize features using the same scaler from training\n",
    "    X_scaled_test = scaler.transform(X_numeric_test)\n",
    "    \n",
    "    # Predict clusters for test data\n",
    "    test_clusters = kmeans.predict(X_scaled_test)\n",
    "    \n",
    "    # Predict using cluster-specific models\n",
    "    predictions = np.zeros(X_scaled_test.shape[0])\n",
    "    \n",
    "    for cluster_id in np.unique(test_clusters):\n",
    "        cluster_idx = test_clusters == cluster_id\n",
    "        X_cluster_test = X_scaled_test[cluster_idx]\n",
    "        \n",
    "        if cluster_id in cluster_models:\n",
    "            model = cluster_models[cluster_id]\n",
    "            # Apply feature selection consistent with training\n",
    "            selected_indices = feature_indices_per_cluster.get(cluster_id, None)\n",
    "            if selected_indices is not None:\n",
    "                X_cluster_test = X_cluster_test[:, selected_indices]\n",
    "            \n",
    "            predictions[cluster_idx] = model.predict(X_cluster_test)\n",
    "            print(f\"Made predictions for {np.sum(cluster_idx)} samples in cluster {cluster_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: No model for cluster {cluster_id}, using average prediction\")\n",
    "            # Use average of all predictions for this cluster\n",
    "            cluster_means = np.mean([m.predict(X_cluster_test[:, feature_indices_per_cluster.get(c_id, slice(None))]).mean() \n",
    "                                    for c_id, m in cluster_models.items()])\n",
    "            predictions[cluster_idx] = cluster_means\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    test_df_with_preds = test_df.copy()\n",
    "    test_df_with_preds['predicted_expense'] = predictions\n",
    "    test_df_with_preds['assigned_cluster'] = test_clusters\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(test_df_with_preds[['predicted_expense', 'assigned_cluster']].head())\n",
    "    \n",
    "    return test_df_with_preds\n",
    "\n",
    "# Aggregate by household ID\n",
    "def aggregate_household_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by household ID\n",
    "    \"\"\"\n",
    "    if 'HH_ID' not in test_results.columns:\n",
    "        print(\"Warning: 'HH_ID' column not found. Cannot aggregate by household.\")\n",
    "        return test_results\n",
    "    \n",
    "    # Calculate the sum of predicted_expense for each household ID\n",
    "    household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
    "    \n",
    "    # Replace each row's predicted_expense with its household sum\n",
    "    test_results_aggregated = test_results.copy()\n",
    "    test_results_aggregated['predicted_expense'] = test_results_aggregated['HH_ID'].map(household_sums)\n",
    "    \n",
    "    # Verify the result with a sample household\n",
    "    sample_household = test_results_aggregated['HH_ID'].iloc[0]\n",
    "    sample_rows = test_results_aggregated[test_results_aggregated['HH_ID'] == sample_household].head()\n",
    "    print(f\"\\nSample rows from household {sample_household}:\")\n",
    "    print(sample_rows[['HH_ID', 'predicted_expense', 'assigned_cluster']])\n",
    "    \n",
    "    return test_results_aggregated\n",
    "\n",
    "# Compare predictions with actual values\n",
    "def compare_expenses(predictions_df, actual_df, \n",
    "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
    "                     join_col='HH_ID'):\n",
    "    \"\"\"\n",
    "    Compare predicted expenses with actual expenses\n",
    "    \"\"\"\n",
    "    # Check if the datasets have the required columns\n",
    "    required_cols = {\n",
    "        'predictions': [pred_col, join_col],\n",
    "        'actual': [actual_col, join_col]\n",
    "    }\n",
    "    \n",
    "    for df_name, cols in required_cols.items():\n",
    "        df = predictions_df if df_name == 'predictions' else actual_df\n",
    "        missing = [col for col in cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
    "    \n",
    "    # If we have multiple rows per household, keep only one row per household in each dataset\n",
    "    pred_unique = predictions_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
    "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
    "    \n",
    "    # Print the number of households we can compare\n",
    "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
    "    \n",
    "    # Extract the values for comparison\n",
    "    y_pred = merged_df[pred_col].values\n",
    "    y_true = merged_df[actual_col].values\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    metrics = evaluate_predictions(y_true, y_pred, method_name=\"Final Evaluation\")\n",
    "    \n",
    "    # Save the comparison to a new CSV for further analysis\n",
    "    merged_df['error'] = y_pred - y_true\n",
    "    merged_df['percentage_error'] = (merged_df['error'] / merged_df[actual_col]) * 100\n",
    "    merged_df.to_csv('expense_comparison_results.csv', index=False)\n",
    "    print(\"\\nDetailed comparison saved to 'expense_comparison_results.csv'\")\n",
    "    \n",
    "    return metrics, merged_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load datasets with paths from the provided code\n",
    "    print(\"Loading datasets...\")\n",
    "    train_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv'  # Using the path from the provided code\n",
    "    test_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/test_data_final.csv'  # Using the path from the provided code\n",
    "    \n",
    "    try:\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Testing data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Print information about null values in test data\n",
    "        null_counts = test_data.isnull().sum()\n",
    "        print(\"\\nNull value counts in test data:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        if null_counts.sum() == 0:\n",
    "            print(\"No null values found in test data\")\n",
    "        \n",
    "        # 2. Fixed parameters\n",
    "        target_column = 'WeightedExpense'\n",
    "        model_type = 'xgboost'  \n",
    "        \n",
    "        # 3. Train cluster-wise models with reduced number of clusters (5)\n",
    "        print(f\"\\nTraining cluster-wise {model_type} models...\")\n",
    "        preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k, feature_indices_per_cluster = cluster_and_predict(\n",
    "            train_data, \n",
    "            target_column=target_column,\n",
    "            max_clusters=10,\n",
    "            model_type=model_type,\n",
    "            manual_clusters=5  # Manually use 5 clusters instead of elbow method\n",
    "        )\n",
    "\n",
    "        # 4. Make predictions on test data\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_with_predictions = predict_on_test_data(\n",
    "            test_data,\n",
    "            kmeans,\n",
    "            cluster_models,\n",
    "            scaler,\n",
    "            numeric_cols,\n",
    "            non_numeric_cols,\n",
    "            feature_indices_per_cluster\n",
    "        )\n",
    "\n",
    "        # 5. Aggregate predictions by household ID\n",
    "        print(\"\\nAggregating predictions by household...\")\n",
    "        final_results = aggregate_household_predictions(test_with_predictions)\n",
    "\n",
    "        # 6. Save the final results\n",
    "        output_path = f'test_predictions_{model_type}_regularized.csv'\n",
    "        final_results.to_csv(output_path, index=False)\n",
    "        print(f\"\\nFinal results saved to {output_path}\")\n",
    "\n",
    "        # 7. Compare with actual values (since we're loading final_test_dataset.csv which should have actual values)\n",
    "        if 'TotalExpense' in test_data.columns:\n",
    "            print(\"\\nActual values found in test data. Comparing predictions...\")\n",
    "            metrics, comparison_df = compare_expenses(\n",
    "                final_results,\n",
    "                test_data,\n",
    "                pred_col='predicted_expense',\n",
    "                actual_col='TotalExpense',\n",
    "                join_col='HH_ID'\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nNo 'TotalExpense' column found in test data. Skipping comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training data shape: (901723, 50)\n",
      "Testing data shape: (225316, 50)\n",
      "\n",
      "Null value counts in test data:\n",
      "Series([], dtype: int64)\n",
      "No null values found in test data\n",
      "\n",
      "Training cluster-wise stacked models...\n",
      "Optimal number of clusters (Elbow method): 7\n",
      "Training stacked model for cluster 0 with 170811 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 0:\n",
      "                    Feature  Importance\n",
      "0            Person Srl No.    0.132419\n",
      "22        HH Size (For FDQ)    0.129969\n",
      "1   Relation to head (code)    0.100957\n",
      "24                   NIC_5D    0.061566\n",
      "3             Age(in years)    0.053910\n",
      "\n",
      "Top 5 features for xgboost in cluster 0:\n",
      "                       Feature  Importance\n",
      "0               Person Srl No.    0.240340\n",
      "22           HH Size (For FDQ)    0.080720\n",
      "45     Is_HH_Have_Refrigerator    0.059460\n",
      "1      Relation to head (code)    0.056594\n",
      "46  Is_HH_Have_Washing_machine    0.048659\n",
      "\n",
      "Top 5 features for lightgbm in cluster 0:\n",
      "                                              Feature  Importance\n",
      "16                                              State         320\n",
      "18                                           District         315\n",
      "22                                  HH Size (For FDQ)         243\n",
      "17                                         NSS-Region         241\n",
      "13  No. of meals taken during last 30 days on payment         207\n",
      "Training stacked model for cluster 1 with 221499 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 1:\n",
      "              Feature  Importance\n",
      "18           District    0.107002\n",
      "17         NSS-Region    0.092360\n",
      "22  HH Size (For FDQ)    0.089114\n",
      "24             NIC_5D    0.083437\n",
      "3       Age(in years)    0.069685\n",
      "\n",
      "Top 5 features for xgboost in cluster 1:\n",
      "                          Feature  Importance\n",
      "45        Is_HH_Have_Refrigerator    0.162994\n",
      "22              HH Size (For FDQ)    0.052001\n",
      "42   Is_HH_Have_Motorcar_jeep_van    0.051991\n",
      "36          Is_HH_Have_Television    0.045130\n",
      "41  Is_HH_Have_Motorcycle_scooter    0.045014\n",
      "\n",
      "Top 5 features for lightgbm in cluster 1:\n",
      "              Feature  Importance\n",
      "18           District         458\n",
      "16              State         395\n",
      "17         NSS-Region         320\n",
      "22  HH Size (For FDQ)         217\n",
      "3       Age(in years)         139\n",
      "Training stacked model for cluster 2 with 119558 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 2:\n",
      "                    Feature  Importance\n",
      "22        HH Size (For FDQ)    0.126328\n",
      "3             Age(in years)    0.092126\n",
      "1   Relation to head (code)    0.085538\n",
      "0            Person Srl No.    0.083067\n",
      "18                 District    0.074336\n",
      "\n",
      "Top 5 features for xgboost in cluster 2:\n",
      "                                              Feature  Importance\n",
      "0                                      Person Srl No.    0.191149\n",
      "22                                  HH Size (For FDQ)    0.065539\n",
      "42                       Is_HH_Have_Motorcar_jeep_van    0.060616\n",
      "46                         Is_HH_Have_Washing_machine    0.045552\n",
      "11  No. of meals taken during last 30 days from em...    0.033243\n",
      "\n",
      "Top 5 features for lightgbm in cluster 2:\n",
      "              Feature  Importance\n",
      "18           District         415\n",
      "22  HH Size (For FDQ)         325\n",
      "16              State         243\n",
      "3       Age(in years)         201\n",
      "24             NIC_5D         178\n",
      "Training stacked model for cluster 3 with 163831 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 3:\n",
      "                    Feature  Importance\n",
      "18                 District    0.107087\n",
      "3             Age(in years)    0.104129\n",
      "17               NSS-Region    0.089408\n",
      "22        HH Size (For FDQ)    0.085690\n",
      "1   Relation to head (code)    0.068747\n",
      "\n",
      "Top 5 features for xgboost in cluster 3:\n",
      "                                              Feature  Importance\n",
      "0                                      Person Srl No.    0.184640\n",
      "22                                  HH Size (For FDQ)    0.050548\n",
      "7   Whether used internet from any location during...    0.048634\n",
      "25               Is_online_Clothing_Purchased_Last365    0.042322\n",
      "5           Highest educational level attained (code)    0.040728\n",
      "\n",
      "Top 5 features for lightgbm in cluster 3:\n",
      "              Feature  Importance\n",
      "18           District         423\n",
      "16              State         307\n",
      "22  HH Size (For FDQ)         302\n",
      "17         NSS-Region         269\n",
      "23             NCO_3D         123\n",
      "Training stacked model for cluster 4 with 87550 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 4:\n",
      "                    Feature  Importance\n",
      "22        HH Size (For FDQ)    0.139393\n",
      "1   Relation to head (code)    0.074046\n",
      "0            Person Srl No.    0.070954\n",
      "3             Age(in years)    0.070589\n",
      "24                   NIC_5D    0.068641\n",
      "\n",
      "Top 5 features for xgboost in cluster 4:\n",
      "                                Feature  Importance\n",
      "0                        Person Srl No.    0.166646\n",
      "42         Is_HH_Have_Motorcar_jeep_van    0.078829\n",
      "22                    HH Size (For FDQ)    0.071321\n",
      "38                 Is_HH_Have_Laptop_PC    0.039813\n",
      "47  Is_HH_Have_Airconditioner_aircooler    0.032921\n",
      "\n",
      "Top 5 features for lightgbm in cluster 4:\n",
      "              Feature  Importance\n",
      "18           District         321\n",
      "22  HH Size (For FDQ)         269\n",
      "16              State         256\n",
      "23             NCO_3D         217\n",
      "24             NIC_5D         199\n",
      "Training stacked model for cluster 5 with 14874 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 5:\n",
      "                         Feature  Importance\n",
      "22             HH Size (For FDQ)    0.130342\n",
      "42  Is_HH_Have_Motorcar_jeep_van    0.087196\n",
      "23                        NCO_3D    0.071835\n",
      "1        Relation to head (code)    0.064049\n",
      "3                  Age(in years)    0.054536\n",
      "\n",
      "Top 5 features for xgboost in cluster 5:\n",
      "                                              Feature  Importance\n",
      "46                         Is_HH_Have_Washing_machine    0.139834\n",
      "42                       Is_HH_Have_Motorcar_jeep_van    0.119322\n",
      "38                               Is_HH_Have_Laptop_PC    0.069219\n",
      "22                                  HH Size (For FDQ)    0.055239\n",
      "8   No. of days stayed away from home during last ...    0.053595\n",
      "\n",
      "Top 5 features for lightgbm in cluster 5:\n",
      "              Feature  Importance\n",
      "18           District         291\n",
      "23             NCO_3D         290\n",
      "22  HH Size (For FDQ)         290\n",
      "24             NIC_5D         219\n",
      "16              State         213\n",
      "Training stacked model for cluster 6 with 123600 samples\n",
      "Training base model: random_forest\n",
      "Training base model: xgboost\n",
      "Training base model: lightgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for random_forest in cluster 6:\n",
      "              Feature  Importance\n",
      "22  HH Size (For FDQ)    0.090357\n",
      "3       Age(in years)    0.085026\n",
      "18           District    0.073536\n",
      "24             NIC_5D    0.065570\n",
      "17         NSS-Region    0.060186\n",
      "\n",
      "Top 5 features for xgboost in cluster 6:\n",
      "                       Feature  Importance\n",
      "45     Is_HH_Have_Refrigerator    0.150747\n",
      "0               Person Srl No.    0.100554\n",
      "46  Is_HH_Have_Washing_machine    0.075944\n",
      "22           HH Size (For FDQ)    0.051849\n",
      "1      Relation to head (code)    0.044064\n",
      "\n",
      "Top 5 features for lightgbm in cluster 6:\n",
      "              Feature  Importance\n",
      "18           District         397\n",
      "17         NSS-Region         290\n",
      "16              State         271\n",
      "22  HH Size (For FDQ)         262\n",
      "3       Age(in years)         156\n",
      "\n",
      "--- Cluster-wise Stacked Model Prediction ---\n",
      "Mean Percentage Error (MPE): 16.75%\n",
      "Mean Absolute Error (MAE): 822.24\n",
      "R² Score: 0.8276\n",
      "\n",
      "--- Cluster Mean Assignment ---\n",
      "Mean Percentage Error (MPE): 46.71%\n",
      "Mean Absolute Error (MAE): 1922.44\n",
      "R² Score: 0.2312\n",
      "\n",
      "Making predictions on test data...\n",
      "All required numeric columns found in test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 42075 samples in cluster 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 55738 samples in cluster 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 29688 samples in cluster 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 41565 samples in cluster 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 21690 samples in cluster 4\n",
      "Made predictions for 3707 samples in cluster 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishav/Documents/University_courses/Sem_6/webai/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions for 30853 samples in cluster 6\n",
      "\n",
      "Sample Predictions:\n",
      "   predicted_expense  assigned_cluster\n",
      "0        6582.274325                 3\n",
      "1        3931.624001                 2\n",
      "2        3846.261876                 2\n",
      "3        3094.223577                 2\n",
      "4        8172.224191                 4\n",
      "\n",
      "Aggregating predictions by household...\n",
      "\n",
      "Sample rows from household HCES2022655561010131113011101202304:\n",
      "                                 HH_ID  predicted_expense  assigned_cluster\n",
      "0  HCES2022655561010131113011101202304        17454.38378                 3\n",
      "1  HCES2022655561010131113011101202304        17454.38378                 2\n",
      "2  HCES2022655561010131113011101202304        17454.38378                 2\n",
      "3  HCES2022655561010131113011101202304        17454.38378                 2\n",
      "\n",
      "Final results saved to test_predictions_stacked_model.csv\n",
      "\n",
      "Actual values found in test data. Comparing predictions...\n",
      "Number of households for comparison: 52350\n",
      "\n",
      "--- Final Evaluation ---\n",
      "Mean Percentage Error (MPE): 26.67%\n",
      "Mean Absolute Error (MAE): 5131.26\n",
      "R² Score: 0.5646\n",
      "\n",
      "Detailed comparison saved to 'expense_comparison_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Standardize numeric features, exclude non-numeric (e.g., IDs)\n",
    "def standardize_features(df, target_column):\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    if target_column in non_numeric_cols:\n",
    "        non_numeric_cols.remove(target_column)\n",
    "\n",
    "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
    "    y = df[target_column].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist()\n",
    "\n",
    "# Elbow method to find optimal clusters\n",
    "def elbow_method_auto(X, max_clusters=10):\n",
    "    inertias = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    # Find elbow: largest drop in inertia\n",
    "    deltas = np.diff(inertias)\n",
    "    second_deltas = np.diff(deltas)\n",
    "    elbow_point = np.argmin(second_deltas) + 2  # +2 accounts for double diff index shift\n",
    "    return elbow_point\n",
    "\n",
    "# Mean Percentage Error\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "    return np.nan\n",
    "\n",
    "# Evaluate and print metrics\n",
    "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
    "    mpe = mean_percentage_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n--- {method_name} ---\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mpe': mpe,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Get base models for stacking\n",
    "def get_base_models():\n",
    "    models = {\n",
    "        'random_forest': RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "        'xgboost': XGBRegressor(random_state=42, n_estimators=100, verbosity=0),\n",
    "        'lightgbm': LGBMRegressor(random_state=42, n_estimators=100, verbose=-1)\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# Get meta model for stacking\n",
    "def get_meta_model():\n",
    "    return LinearRegression()\n",
    "\n",
    "# Stacked model implementation for a specific cluster\n",
    "def train_stacked_model(X, y, n_folds=5):\n",
    "    # Setup K-fold cross-validation\n",
    "    kfold = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Get base models and meta model\n",
    "    base_models = get_base_models()\n",
    "    meta_model = get_meta_model()\n",
    "    \n",
    "    # Placeholder for meta features (predictions from base models)\n",
    "    meta_features = np.zeros((X.shape[0], len(base_models)))\n",
    "    \n",
    "    # Train base models and generate meta features\n",
    "    for i, (model_name, model) in enumerate(base_models.items()):\n",
    "        print(f\"Training base model: {model_name}\")\n",
    "        # Use cross-validation to create out-of-fold predictions\n",
    "        oof_predictions = np.zeros(X.shape[0])\n",
    "        \n",
    "        for train_idx, val_idx in kfold.split(X):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            oof_predictions[val_idx] = model.predict(X_val)\n",
    "        \n",
    "        # Store out-of-fold predictions as meta features\n",
    "        meta_features[:, i] = oof_predictions\n",
    "        \n",
    "        # Retrain the model on the full data\n",
    "        model.fit(X, y)\n",
    "    \n",
    "    # Train meta model on the meta features\n",
    "    meta_model.fit(meta_features, y)\n",
    "    \n",
    "    # Create a stacked model dictionary containing all models\n",
    "    stacked_model = {\n",
    "        'base_models': base_models,\n",
    "        'meta_model': meta_model\n",
    "    }\n",
    "    \n",
    "    return stacked_model\n",
    "\n",
    "# Main function: cluster, predict, evaluate using stacked model\n",
    "def cluster_and_predict_stacked(train_df, target_column='WeightedExpense', max_clusters=10):\n",
    "    \"\"\"\n",
    "    Implement clustering approach with stacked model for each cluster\n",
    "    \"\"\"\n",
    "    # Step 1: Standardize numeric features\n",
    "    X_scaled, y, scaler, non_numeric_cols, numeric_cols = standardize_features(train_df, target_column)\n",
    "\n",
    "    # Step 2: Find optimal clusters\n",
    "    optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
    "    print(f\"Optimal number of clusters (Elbow method): {optimal_k}\")\n",
    "\n",
    "    # Step 3: KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Add cluster labels to DataFrame for reference\n",
    "    df_with_clusters = train_df.copy()\n",
    "    df_with_clusters['Cluster'] = clusters\n",
    "\n",
    "    # Step 4: Cluster-wise stacked model training\n",
    "    preds_model = np.zeros_like(y)\n",
    "    cluster_models = {}\n",
    "    \n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        X_cluster = X_scaled[cluster_idx]\n",
    "        y_cluster = y[cluster_idx]\n",
    "        \n",
    "        print(f\"Training stacked model for cluster {cluster_id} with {len(y_cluster)} samples\")\n",
    "        \n",
    "        # Skip empty clusters (shouldn't happen but just in case)\n",
    "        if len(y_cluster) < n_folds:\n",
    "            print(f\"Skipping cluster {cluster_id} - not enough samples for {n_folds}-fold CV\")\n",
    "            continue\n",
    "\n",
    "        # Train stacked model for this cluster\n",
    "        stacked_model = train_stacked_model(X_cluster, y_cluster)\n",
    "        \n",
    "        # Make predictions for this cluster\n",
    "        # First get base model predictions\n",
    "        base_preds = np.zeros((X_cluster.shape[0], len(stacked_model['base_models'])))\n",
    "        for i, (name, model) in enumerate(stacked_model['base_models'].items()):\n",
    "            base_preds[:, i] = model.predict(X_cluster)\n",
    "            \n",
    "        # Then use meta model to make final predictions\n",
    "        preds_model[cluster_idx] = stacked_model['meta_model'].predict(base_preds)\n",
    "\n",
    "        # Save stacked model per cluster\n",
    "        cluster_models[cluster_id] = stacked_model\n",
    "        \n",
    "        # Print feature importance for Random Forest and tree-based models\n",
    "        for name, model in stacked_model['base_models'].items():\n",
    "            if hasattr(model, 'feature_importances_') and len(numeric_cols) > 0:\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': numeric_cols,\n",
    "                    'Importance': model.feature_importances_\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\nTop 5 features for {name} in cluster {cluster_id}:\")\n",
    "                print(feature_importance.head(5))\n",
    "\n",
    "    # Evaluate cluster-wise stacked model\n",
    "    evaluate_predictions(y, preds_model, method_name=\"Cluster-wise Stacked Model Prediction\")\n",
    "\n",
    "    # Step 4B: Cluster mean assignment (as a simple baseline)\n",
    "    preds_mean = np.zeros_like(y)\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_idx = clusters == cluster_id\n",
    "        y_cluster = y[cluster_idx]\n",
    "        cluster_mean = np.mean(y_cluster)\n",
    "        preds_mean[cluster_idx] = cluster_mean\n",
    "\n",
    "    # Evaluate cluster mean assignment\n",
    "    evaluate_predictions(y, preds_mean, method_name=\"Cluster Mean Assignment\")\n",
    "\n",
    "    return preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k\n",
    "\n",
    "# Predict on test data using cluster-specific stacked models\n",
    "def predict_on_test_data_stacked(test_df, kmeans, cluster_models, scaler, numeric_cols, non_numeric_cols):\n",
    "    \"\"\"\n",
    "    Predict expenses on test data using trained cluster-specific stacked models\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with only the numeric columns in the correct order\n",
    "    X_numeric_test = pd.DataFrame(index=test_df.index)\n",
    "    \n",
    "    # Track any columns missing from test data\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in test_df.columns:\n",
    "            # Check if this specific column has nulls before adding it\n",
    "            if test_df[col].isnull().any():\n",
    "                # Fill nulls in this column with its median before adding\n",
    "                X_numeric_test[col] = test_df[col].fillna(test_df[col].median())\n",
    "                print(f\"Filled nulls in column '{col}' with its median\")\n",
    "            else:\n",
    "                X_numeric_test[col] = test_df[col]\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "            print(f\"Warning: Column '{col}' missing from test data. Using zeros.\")\n",
    "            X_numeric_test[col] = 0\n",
    "    \n",
    "    # Report on missing columns if any\n",
    "    if missing_columns:\n",
    "        print(f\"\\nTotal missing columns: {len(missing_columns)}\")\n",
    "        if len(missing_columns) <= 10:  # Only print if there aren't too many\n",
    "            print(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "        else:\n",
    "            print(f\"First 10 missing columns: {', '.join(missing_columns[:10])}...\")\n",
    "    else:\n",
    "        print(\"All required numeric columns found in test data\")\n",
    "    \n",
    "    # Standardize features using the same scaler from training\n",
    "    X_scaled_test = scaler.transform(X_numeric_test)\n",
    "    \n",
    "    # Predict clusters for test data\n",
    "    test_clusters = kmeans.predict(X_scaled_test)\n",
    "    \n",
    "    # Predict using cluster-specific stacked models\n",
    "    predictions = np.zeros(X_scaled_test.shape[0])\n",
    "    \n",
    "    for cluster_id in np.unique(test_clusters):\n",
    "        cluster_idx = test_clusters == cluster_id\n",
    "        X_cluster_test = X_scaled_test[cluster_idx]\n",
    "        \n",
    "        if cluster_id in cluster_models:\n",
    "            stacked_model = cluster_models[cluster_id]\n",
    "            \n",
    "            # Get predictions from base models\n",
    "            base_preds = np.zeros((X_cluster_test.shape[0], len(stacked_model['base_models'])))\n",
    "            for i, (name, model) in enumerate(stacked_model['base_models'].items()):\n",
    "                base_preds[:, i] = model.predict(X_cluster_test)\n",
    "            \n",
    "            # Use meta model for final prediction\n",
    "            cluster_preds = stacked_model['meta_model'].predict(base_preds)\n",
    "            predictions[cluster_idx] = cluster_preds\n",
    "            \n",
    "            print(f\"Made predictions for {np.sum(cluster_idx)} samples in cluster {cluster_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: No model for cluster {cluster_id}, using average prediction\")\n",
    "            # Use average of all predictions for this cluster\n",
    "            all_preds = []\n",
    "            for c_id, stacked_model in cluster_models.items():\n",
    "                # Get base model predictions\n",
    "                base_preds = np.zeros((X_cluster_test.shape[0], len(stacked_model['base_models'])))\n",
    "                for i, (name, model) in enumerate(stacked_model['base_models'].items()):\n",
    "                    base_preds[:, i] = model.predict(X_cluster_test)\n",
    "                \n",
    "                # Get meta model predictions\n",
    "                c_preds = stacked_model['meta_model'].predict(base_preds)\n",
    "                all_preds.append(np.mean(c_preds))\n",
    "            \n",
    "            # Use average of all cluster predictions\n",
    "            predictions[cluster_idx] = np.mean(all_preds)\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    test_df_with_preds = test_df.copy()\n",
    "    test_df_with_preds['predicted_expense'] = predictions\n",
    "    test_df_with_preds['assigned_cluster'] = test_clusters\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(test_df_with_preds[['predicted_expense', 'assigned_cluster']].head())\n",
    "    \n",
    "    return test_df_with_preds\n",
    "\n",
    "# Aggregate by household ID\n",
    "def aggregate_household_predictions(test_results):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by household ID\n",
    "    \"\"\"\n",
    "    if 'HH_ID' not in test_results.columns:\n",
    "        print(\"Warning: 'HH_ID' column not found. Cannot aggregate by household.\")\n",
    "        return test_results\n",
    "    \n",
    "    # Calculate the sum of predicted_expense for each household ID\n",
    "    household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
    "    \n",
    "    # Replace each row's predicted_expense with its household sum\n",
    "    test_results_aggregated = test_results.copy()\n",
    "    test_results_aggregated['predicted_expense'] = test_results_aggregated['HH_ID'].map(household_sums)\n",
    "    \n",
    "    # Verify the result with a sample household\n",
    "    sample_household = test_results_aggregated['HH_ID'].iloc[0]\n",
    "    sample_rows = test_results_aggregated[test_results_aggregated['HH_ID'] == sample_household].head()\n",
    "    print(f\"\\nSample rows from household {sample_household}:\")\n",
    "    print(sample_rows[['HH_ID', 'predicted_expense', 'assigned_cluster']])\n",
    "    \n",
    "    return test_results_aggregated\n",
    "\n",
    "# Compare predictions with actual values\n",
    "def compare_expenses(predictions_df, actual_df, \n",
    "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
    "                     join_col='HH_ID'):\n",
    "    \"\"\"\n",
    "    Compare predicted expenses with actual expenses\n",
    "    \"\"\"\n",
    "    # Check if the datasets have the required columns\n",
    "    required_cols = {\n",
    "        'predictions': [pred_col, join_col],\n",
    "        'actual': [actual_col, join_col]\n",
    "    }\n",
    "    \n",
    "    for df_name, cols in required_cols.items():\n",
    "        df = predictions_df if df_name == 'predictions' else actual_df\n",
    "        missing = [col for col in cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
    "    \n",
    "    # If we have multiple rows per household, keep only one row per household in each dataset\n",
    "    pred_unique = predictions_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
    "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
    "    \n",
    "    # Print the number of households we can compare\n",
    "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
    "    \n",
    "    # Extract the values for comparison\n",
    "    y_pred = merged_df[pred_col].values\n",
    "    y_true = merged_df[actual_col].values\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    metrics = evaluate_predictions(y_true, y_pred, method_name=\"Final Evaluation\")\n",
    "    \n",
    "    # Save the comparison to a new CSV for further analysis\n",
    "    merged_df['error'] = y_pred - y_true\n",
    "    merged_df['percentage_error'] = (merged_df['error'] / merged_df[actual_col]) * 100\n",
    "    merged_df.to_csv('expense_comparison_results.csv', index=False)\n",
    "    print(\"\\nDetailed comparison saved to 'expense_comparison_results.csv'\")\n",
    "    \n",
    "    return metrics, merged_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load datasets with paths from the provided code\n",
    "    print(\"Loading datasets...\")\n",
    "    train_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv'  # Using the path from the provided code\n",
    "    test_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/test_data_final.csv'  # Using the path from the provided code\n",
    "    \n",
    "    try:\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Testing data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Print information about null values in test data\n",
    "        null_counts = test_data.isnull().sum()\n",
    "        print(\"\\nNull value counts in test data:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        if null_counts.sum() == 0:\n",
    "            print(\"No null values found in test data\")\n",
    "        \n",
    "        # 2. Fixed parameters\n",
    "        target_column = 'WeightedExpense'\n",
    "        n_folds = 5  # Number of folds for cross-validation in stacking\n",
    "        \n",
    "        # 3. Train cluster-wise stacked models\n",
    "        print(f\"\\nTraining cluster-wise stacked models...\")\n",
    "        preds_model, preds_mean, cluster_models, kmeans, scaler, numeric_cols, non_numeric_cols, clusters, optimal_k = cluster_and_predict_stacked(\n",
    "            train_data, \n",
    "            target_column=target_column,\n",
    "            max_clusters=10\n",
    "        )\n",
    "\n",
    "        # 4. Make predictions on test data using stacked models\n",
    "        print(\"\\nMaking predictions on test data...\")\n",
    "        test_with_predictions = predict_on_test_data_stacked(\n",
    "            test_data,\n",
    "            kmeans,\n",
    "            cluster_models,\n",
    "            scaler,\n",
    "            numeric_cols,\n",
    "            non_numeric_cols\n",
    "        )\n",
    "\n",
    "        # 5. Aggregate predictions by household ID\n",
    "        print(\"\\nAggregating predictions by household...\")\n",
    "        final_results = aggregate_household_predictions(test_with_predictions)\n",
    "\n",
    "        # 6. Save the final results\n",
    "        output_path = 'test_predictions_stacked_model.csv'\n",
    "        final_results.to_csv(output_path, index=False)\n",
    "        print(f\"\\nFinal results saved to {output_path}\")\n",
    "\n",
    "        # 7. Compare with actual values (since we're loading final_test_dataset.csv which should have actual values)\n",
    "        if 'TotalExpense' in test_data.columns:\n",
    "            print(\"\\nActual values found in test data. Comparing predictions...\")\n",
    "            metrics, comparison_df = compare_expenses(\n",
    "                final_results,\n",
    "                test_data,\n",
    "                pred_col='predicted_expense',\n",
    "                actual_col='TotalExpense',\n",
    "                join_col='HH_ID'\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nNo 'TotalExpense' column found in test data. Skipping comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
