{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Mgziw5WTqU",
        "outputId": "5045a8de-83f8-463f-b542-a4366447845e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Household Train Data Shape: (209396, 35)\n",
            "Person Train Data Shape: (901723, 16)\n",
            "\n",
            "Missing Values in Household Train Data:\n",
            "HH_ID                                                    0\n",
            "Sector                                                   0\n",
            "State                                                    0\n",
            "NSS-Region                                               0\n",
            "District                                                 0\n",
            "Household Type                                           0\n",
            "Religion of the head of the household                    0\n",
            "Social Group of the head of the household                0\n",
            "HH Size (For FDQ)                                        0\n",
            "NCO_3D                                               19947\n",
            "NIC_5D                                               19947\n",
            "Is_online_Clothing_Purchased_Last365                167602\n",
            "Is_online_Footwear_Purchased_Last365                181550\n",
            "Is_online_Furniture_fixturesPurchased_Last365       208560\n",
            "Is_online_Mobile_Handset_Purchased_Last365          205356\n",
            "Is_online_Personal_Goods_Purchased_Last365          202783\n",
            "Is_online_Recreation_Goods_Purchased_Last365        207638\n",
            "Is_online_Household_Appliances_Purchased_Last365    206608\n",
            "Is_online_Crockery_Utensils_Purchased_Last365       205879\n",
            "Is_online_Sports_Goods_Purchased_Last365            206820\n",
            "Is_online_Medical_Equipment_Purchased_Last365       208091\n",
            "Is_online_Bedding_Purchased_Last365                 204260\n",
            "Is_HH_Have_Television                                64631\n",
            "Is_HH_Have_Radio                                    202801\n",
            "Is_HH_Have_Laptop_PC                                192223\n",
            "Is_HH_Have_Mobile_handset                             6881\n",
            "Is_HH_Have_Bicycle                                  124625\n",
            "Is_HH_Have_Motorcycle_scooter                        94988\n",
            "Is_HH_Have_Motorcar_jeep_van                        191938\n",
            "Is_HH_Have_Trucks                                   209063\n",
            "Is_HH_Have_Animal_cart                              207783\n",
            "Is_HH_Have_Refrigerator                             118296\n",
            "Is_HH_Have_Washing_machine                          163375\n",
            "Is_HH_Have_Airconditioner_aircooler                 157674\n",
            "TotalExpense                                             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "#import xgboost as xgb\n",
        "\n",
        "# Load the data\n",
        "# Replace with your actual file paths\n",
        "hh_train = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Train_Data/HH_Level_train_data.csv')\n",
        "person_train = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Train_Data/Person_Level_Train_Data.csv')\n",
        "hh_test = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Test_Data/HH_Level_test_data.csv')\n",
        "person_test = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Test_Data/Person_Level_Test_Data.csv')\n",
        "# Basic information about the datasets\n",
        "print(\"Household Train Data Shape:\", hh_train.shape)\n",
        "print(\"Person Train Data Shape:\", person_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values in Household Train Data:\")\n",
        "print(hh_train.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSI0ipmcnKyg"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD91-8mKWTqW",
        "outputId": "148ebf90-a8a7-4f48-f4c6-a6cb276cff5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values in Person Level Data:\n",
            "HH_ID                                                                                       0\n",
            "Person Srl No.                                                                              0\n",
            "Relation to head (code)                                                                     0\n",
            "Gender                                                                                      0\n",
            "Age(in years)                                                                               0\n",
            "Marital Status (code)                                                                       0\n",
            "Highest educational level attained (code)                                                   0\n",
            "Total year of education completed                                                      225096\n",
            "Whether used internet from any location during last 30 days                             32034\n",
            "No. of days stayed away from home during last 30 days                                    4617\n",
            "No. of meals usually taken in a day                                                      2385\n",
            "No. of meals taken during last 30 days from school, balwadi etc.                       586364\n",
            "No. of meals taken during last 30 days from employer as perquisites or part of wage    620885\n",
            "No. of meals taken during last 30 days  others                                         556701\n",
            "No. of meals taken during last 30 days on payment                                      576961\n",
            "No. of meals taken during last 30 days at home                                           6708\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMissing Values in Person Level Data:\")\n",
        "print(person_train.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1uWCh84pWTqW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "# Replace with your actual file paths\n",
        "hh_train = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Train_Data/HH_Level_train_data.csv')\n",
        "person_train = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Train_Data/Person_Level_Train_Data.csv')\n",
        "hh_test = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Test_Data/HH_Level_test_data.csv')\n",
        "person_test = pd.read_csv('/Users/rishav/Downloads/IIT_GN/Test_Data/Person_Level_Test_Data.csv')\n",
        "\n",
        "# Define the binary categorical columns\n",
        "binary_categorical_columns = [\n",
        "    'Is_online_Clothing_Purchased_Last365',\n",
        "    'Is_online_Footwear_Purchased_Last365',\n",
        "    'Is_online_Furniture_fixturesPurchased_Last365',\n",
        "    'Is_online_Mobile_Handset_Purchased_Last365',\n",
        "    'Is_online_Personal_Goods_Purchased_Last365',\n",
        "    'Is_online_Recreation_Goods_Purchased_Last365',\n",
        "    'Is_online_Household_Appliances_Purchased_Last365',\n",
        "    'Is_online_Crockery_Utensils_Purchased_Last365',\n",
        "    'Is_online_Sports_Goods_Purchased_Last365',\n",
        "    'Is_online_Medical_Equipment_Purchased_Last365',\n",
        "    'Is_online_Bedding_Purchased_Last365',\n",
        "    'Is_HH_Have_Television',\n",
        "    'Is_HH_Have_Radio',\n",
        "    'Is_HH_Have_Laptop_PC',\n",
        "    'Is_HH_Have_Mobile_handset',\n",
        "    'Is_HH_Have_Bicycle',\n",
        "    'Is_HH_Have_Motorcycle_scooter',\n",
        "    'Is_HH_Have_Motorcar_jeep_van',\n",
        "    'Is_HH_Have_Trucks',\n",
        "    'Is_HH_Have_Animal_cart',\n",
        "    'Is_HH_Have_Refrigerator',\n",
        "    'Is_HH_Have_Washing_machine',\n",
        "    'Is_HH_Have_Airconditioner_aircooler'\n",
        "]\n",
        "\n",
        "# Fill NA values with 0 (No)\n",
        "for column in binary_categorical_columns:\n",
        "    if column in hh_train.columns:\n",
        "        hh_train[column] = hh_train[column].fillna(0).astype(int)\n",
        "        hh_test[column] = hh_test[column].fillna(0).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7PwTZJzWTqX",
        "outputId": "d3c7c02d-bd09-4a36-ad74-256bf7352fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values in Household Train Data:\n",
            "HH_ID                                                   0\n",
            "Sector                                                  0\n",
            "State                                                   0\n",
            "NSS-Region                                              0\n",
            "District                                                0\n",
            "Household Type                                          0\n",
            "Religion of the head of the household                   0\n",
            "Social Group of the head of the household               0\n",
            "HH Size (For FDQ)                                       0\n",
            "NCO_3D                                              19947\n",
            "NIC_5D                                              19947\n",
            "Is_online_Clothing_Purchased_Last365                    0\n",
            "Is_online_Footwear_Purchased_Last365                    0\n",
            "Is_online_Furniture_fixturesPurchased_Last365           0\n",
            "Is_online_Mobile_Handset_Purchased_Last365              0\n",
            "Is_online_Personal_Goods_Purchased_Last365              0\n",
            "Is_online_Recreation_Goods_Purchased_Last365            0\n",
            "Is_online_Household_Appliances_Purchased_Last365        0\n",
            "Is_online_Crockery_Utensils_Purchased_Last365           0\n",
            "Is_online_Sports_Goods_Purchased_Last365                0\n",
            "Is_online_Medical_Equipment_Purchased_Last365           0\n",
            "Is_online_Bedding_Purchased_Last365                     0\n",
            "Is_HH_Have_Television                                   0\n",
            "Is_HH_Have_Radio                                        0\n",
            "Is_HH_Have_Laptop_PC                                    0\n",
            "Is_HH_Have_Mobile_handset                               0\n",
            "Is_HH_Have_Bicycle                                      0\n",
            "Is_HH_Have_Motorcycle_scooter                           0\n",
            "Is_HH_Have_Motorcar_jeep_van                            0\n",
            "Is_HH_Have_Trucks                                       0\n",
            "Is_HH_Have_Animal_cart                                  0\n",
            "Is_HH_Have_Refrigerator                                 0\n",
            "Is_HH_Have_Washing_machine                              0\n",
            "Is_HH_Have_Airconditioner_aircooler                     0\n",
            "TotalExpense                                            0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing Values in Household Train Data:\")\n",
        "print(hh_train.isnull().sum())\n",
        "#print(hh_test.isnull().sum())   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "toZGYXt-XRFu"
      },
      "outputs": [],
      "source": [
        "merged_train = pd.merge(\n",
        "    person_train,\n",
        "    hh_train,\n",
        "    on='HH_ID',\n",
        "    how='inner'  # Only keep records that exist in both datasets\n",
        ")\n",
        "merged_test = pd.merge(\n",
        "    person_test,\n",
        "    hh_test,\n",
        "    on='HH_ID',\n",
        "    how='inner'  # Only keep records that exist in both datasets\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "wkas0holhimM"
      },
      "outputs": [],
      "source": [
        "merged_train.to_csv('Merged_train_data.csv', index=False)\n",
        "merged_test.to_csv('Merged_test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjP1gcDOhviq",
        "outputId": "564ffd88-b44e-4a3a-ac83-1aef8989ed5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(901723, 50)\n"
          ]
        }
      ],
      "source": [
        "print(merged_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Jd8s8bh36I",
        "outputId": "ce0da52a-5f66-4c52-c727-6928fd3c1d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values in Household Train Data:\n",
            "HH_ID                                                                                       0\n",
            "Person Srl No.                                                                              0\n",
            "Relation to head (code)                                                                     0\n",
            "Gender                                                                                      0\n",
            "Age(in years)                                                                               0\n",
            "Marital Status (code)                                                                       0\n",
            "Highest educational level attained (code)                                                   0\n",
            "Total year of education completed                                                       56853\n",
            "Whether used internet from any location during last 30 days                              8189\n",
            "No. of days stayed away from home during last 30 days                                    1171\n",
            "No. of meals usually taken in a day                                                       614\n",
            "No. of meals taken during last 30 days from school, balwadi etc.                       147146\n",
            "No. of meals taken during last 30 days from employer as perquisites or part of wage    155936\n",
            "No. of meals taken during last 30 days  others                                         139775\n",
            "No. of meals taken during last 30 days on payment                                      145097\n",
            "No. of meals taken during last 30 days at home                                           1704\n",
            "Sector                                                                                      0\n",
            "State                                                                                       0\n",
            "NSS-Region                                                                                  0\n",
            "District                                                                                    0\n",
            "Household Type                                                                              0\n",
            "Religion of the head of the household                                                       0\n",
            "Social Group of the head of the household                                                   0\n",
            "HH Size (For FDQ)                                                                           0\n",
            "NCO_3D                                                                                  12215\n",
            "NIC_5D                                                                                  12215\n",
            "Is_online_Clothing_Purchased_Last365                                                        0\n",
            "Is_online_Footwear_Purchased_Last365                                                        0\n",
            "Is_online_Furniture_fixturesPurchased_Last365                                               0\n",
            "Is_online_Mobile_Handset_Purchased_Last365                                                  0\n",
            "Is_online_Personal_Goods_Purchased_Last365                                                  0\n",
            "Is_online_Recreation_Goods_Purchased_Last365                                                0\n",
            "Is_online_Household_Appliances_Purchased_Last365                                            0\n",
            "Is_online_Crockery_Utensils_Purchased_Last365                                               0\n",
            "Is_online_Sports_Goods_Purchased_Last365                                                    0\n",
            "Is_online_Medical_Equipment_Purchased_Last365                                               0\n",
            "Is_online_Bedding_Purchased_Last365                                                         0\n",
            "Is_HH_Have_Television                                                                       0\n",
            "Is_HH_Have_Radio                                                                            0\n",
            "Is_HH_Have_Laptop_PC                                                                        0\n",
            "Is_HH_Have_Mobile_handset                                                                   0\n",
            "Is_HH_Have_Bicycle                                                                          0\n",
            "Is_HH_Have_Motorcycle_scooter                                                               0\n",
            "Is_HH_Have_Motorcar_jeep_van                                                                0\n",
            "Is_HH_Have_Trucks                                                                           0\n",
            "Is_HH_Have_Animal_cart                                                                      0\n",
            "Is_HH_Have_Refrigerator                                                                     0\n",
            "Is_HH_Have_Washing_machine                                                                  0\n",
            "Is_HH_Have_Airconditioner_aircooler                                                         0\n",
            "TotalExpense                                                                                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing Values in Household Train Data:\")\n",
        "#print(merged_train.isnull().sum())\n",
        "print(merged_test.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZEr1E1ViN8u",
        "outputId": "1108fc3c-647a-49d1-f8dc-3f2d2bba395b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using merged datasets\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/Users/rishav/Downloads/modified_data.csv')\n",
        "test_data = pd.read_csv('Merged_test_data.csv')\n",
        "print(\"Using merged datasets\")\n",
        "\n",
        "# Column names\n",
        "education_level_col = 'Highest educational level attained (code)'\n",
        "education_years_col = 'Total year of education completed'\n",
        "\n",
        "# Function to fill missing education years based on educational level code\n",
        "def fill_missing_education_years(df):\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_filled = df.copy()\n",
        "\n",
        "    # For each educational level code, calculate the average years of education\n",
        "    education_level_groups = df.groupby(education_level_col)[education_years_col].mean().to_dict()\n",
        "\n",
        "\n",
        "    # Fill missing values with the calculated averages\n",
        "    missing_mask = df_filled[education_years_col].isnull()\n",
        "\n",
        "    # Set education years to 0 for education level codes 1 and 2\n",
        "    level1_mask = (df_filled[education_level_col] == 1) & missing_mask\n",
        "    level2_mask = (df_filled[education_level_col] == 2) & missing_mask\n",
        "    df_filled.loc[level1_mask, education_years_col] = 0\n",
        "    df_filled.loc[level2_mask, education_years_col] = 0\n",
        "\n",
        "    # For other education levels, use the average years for that level\n",
        "    for level, avg_years in education_level_groups.items():\n",
        "        if level not in [1, 2]:  # Skip levels 1 and 2 as we've already handled them\n",
        "            level_mask = (df_filled[education_level_col] == level) & missing_mask\n",
        "            df_filled.loc[level_mask, education_years_col] = avg_years\n",
        "\n",
        "    return df_filled\n",
        "\n",
        "# Fill missing values\n",
        "train_data_filled = fill_missing_education_years(train_data)\n",
        "test_data_filled = fill_missing_education_years(test_data)\n",
        "# Check if any missing values remain\n",
        "remaining_missing_train = train_data_filled[education_years_col].isnull().sum()\n",
        "remaining_missing_test = test_data_filled[education_years_col].isnull().sum()\n",
        "\n",
        "\n",
        "train_data_filled.to_csv('Merged_train_data_filled.csv', index=False)\n",
        "test_data_filled.to_csv('Merged_test_data_filled.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2_qvlvHoIgi",
        "outputId": "95216af8-8739-429a-d1f2-7eec2fee0121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values in Merged Train Data:\n",
            "HH_ID                                                                                       0\n",
            "Person Srl No.                                                                              0\n",
            "Relation to head (code)                                                                     0\n",
            "Gender                                                                                      0\n",
            "Age(in years)                                                                               0\n",
            "Marital Status (code)                                                                       0\n",
            "Highest educational level attained (code)                                                   0\n",
            "Total year of education completed                                                      225096\n",
            "Whether used internet from any location during last 30 days                             32034\n",
            "No. of days stayed away from home during last 30 days                                    4617\n",
            "No. of meals usually taken in a day                                                      2385\n",
            "No. of meals taken during last 30 days from school, balwadi etc.                       586364\n",
            "No. of meals taken during last 30 days from employer as perquisites or part of wage    620885\n",
            "No. of meals taken during last 30 days  others                                         556701\n",
            "No. of meals taken during last 30 days on payment                                      576961\n",
            "No. of meals taken during last 30 days at home                                           6708\n",
            "Sector                                                                                      0\n",
            "State                                                                                       0\n",
            "NSS-Region                                                                                  0\n",
            "District                                                                                    0\n",
            "Household Type                                                                              0\n",
            "Religion of the head of the household                                                       0\n",
            "Social Group of the head of the household                                                   0\n",
            "HH Size (For FDQ)                                                                           0\n",
            "NCO_3D                                                                                  50220\n",
            "NIC_5D                                                                                  50220\n",
            "Is_online_Clothing_Purchased_Last365                                                        0\n",
            "Is_online_Footwear_Purchased_Last365                                                        0\n",
            "Is_online_Furniture_fixturesPurchased_Last365                                               0\n",
            "Is_online_Mobile_Handset_Purchased_Last365                                                  0\n",
            "Is_online_Personal_Goods_Purchased_Last365                                                  0\n",
            "Is_online_Recreation_Goods_Purchased_Last365                                                0\n",
            "Is_online_Household_Appliances_Purchased_Last365                                            0\n",
            "Is_online_Crockery_Utensils_Purchased_Last365                                               0\n",
            "Is_online_Sports_Goods_Purchased_Last365                                                    0\n",
            "Is_online_Medical_Equipment_Purchased_Last365                                               0\n",
            "Is_online_Bedding_Purchased_Last365                                                         0\n",
            "Is_HH_Have_Television                                                                       0\n",
            "Is_HH_Have_Radio                                                                            0\n",
            "Is_HH_Have_Laptop_PC                                                                        0\n",
            "Is_HH_Have_Mobile_handset                                                                   0\n",
            "Is_HH_Have_Bicycle                                                                          0\n",
            "Is_HH_Have_Motorcycle_scooter                                                               0\n",
            "Is_HH_Have_Motorcar_jeep_van                                                                0\n",
            "Is_HH_Have_Trucks                                                                           0\n",
            "Is_HH_Have_Animal_cart                                                                      0\n",
            "Is_HH_Have_Refrigerator                                                                     0\n",
            "Is_HH_Have_Washing_machine                                                                  0\n",
            "Is_HH_Have_Airconditioner_aircooler                                                         0\n",
            "TotalExpense                                                                                0\n",
            "dtype: int64\n",
            "HH_ID                                                                                       0\n",
            "Person Srl No.                                                                              0\n",
            "Relation to head (code)                                                                     0\n",
            "Gender                                                                                      0\n",
            "Age(in years)                                                                               0\n",
            "Marital Status (code)                                                                       0\n",
            "Highest educational level attained (code)                                                   0\n",
            "Total year of education completed                                                       56853\n",
            "Whether used internet from any location during last 30 days                              8189\n",
            "No. of days stayed away from home during last 30 days                                    1171\n",
            "No. of meals usually taken in a day                                                       614\n",
            "No. of meals taken during last 30 days from school, balwadi etc.                       147146\n",
            "No. of meals taken during last 30 days from employer as perquisites or part of wage    155936\n",
            "No. of meals taken during last 30 days  others                                         139775\n",
            "No. of meals taken during last 30 days on payment                                      145097\n",
            "No. of meals taken during last 30 days at home                                           1704\n",
            "Sector                                                                                      0\n",
            "State                                                                                       0\n",
            "NSS-Region                                                                                  0\n",
            "District                                                                                    0\n",
            "Household Type                                                                              0\n",
            "Religion of the head of the household                                                       0\n",
            "Social Group of the head of the household                                                   0\n",
            "HH Size (For FDQ)                                                                           0\n",
            "NCO_3D                                                                                  12215\n",
            "NIC_5D                                                                                  12215\n",
            "Is_online_Clothing_Purchased_Last365                                                        0\n",
            "Is_online_Footwear_Purchased_Last365                                                        0\n",
            "Is_online_Furniture_fixturesPurchased_Last365                                               0\n",
            "Is_online_Mobile_Handset_Purchased_Last365                                                  0\n",
            "Is_online_Personal_Goods_Purchased_Last365                                                  0\n",
            "Is_online_Recreation_Goods_Purchased_Last365                                                0\n",
            "Is_online_Household_Appliances_Purchased_Last365                                            0\n",
            "Is_online_Crockery_Utensils_Purchased_Last365                                               0\n",
            "Is_online_Sports_Goods_Purchased_Last365                                                    0\n",
            "Is_online_Medical_Equipment_Purchased_Last365                                               0\n",
            "Is_online_Bedding_Purchased_Last365                                                         0\n",
            "Is_HH_Have_Television                                                                       0\n",
            "Is_HH_Have_Radio                                                                            0\n",
            "Is_HH_Have_Laptop_PC                                                                        0\n",
            "Is_HH_Have_Mobile_handset                                                                   0\n",
            "Is_HH_Have_Bicycle                                                                          0\n",
            "Is_HH_Have_Motorcycle_scooter                                                               0\n",
            "Is_HH_Have_Motorcar_jeep_van                                                                0\n",
            "Is_HH_Have_Trucks                                                                           0\n",
            "Is_HH_Have_Animal_cart                                                                      0\n",
            "Is_HH_Have_Refrigerator                                                                     0\n",
            "Is_HH_Have_Washing_machine                                                                  0\n",
            "Is_HH_Have_Airconditioner_aircooler                                                         0\n",
            "TotalExpense                                                                                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "train_data_final = pd.read_csv('Merged_train_data_filled.csv')\n",
        "test_data_final = pd.read_csv('Merged_test_data_filled.csv')\n",
        "print(\"\\nMissing Values in Merged Train Data:\")\n",
        "print(train_data.isnull().sum())\n",
        "print(test_data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJQhgX0-sXCW",
        "outputId": "ab93ce0c-68d1-4f7a-f4e3-5dcaa0722a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(901723, 50)\n"
          ]
        }
      ],
      "source": [
        "print(train_data_final.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG9nZ63MB1rO",
        "outputId": "b8ff18e7-6e81-4f08-e636-26425cf410ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['HH_ID', 'Person Srl No.', 'Relation to head (code)', 'Gender',\n",
              "       'Age(in years)', 'Marital Status (code)',\n",
              "       'Highest educational level attained (code)',\n",
              "       'Total year of education completed',\n",
              "       'Whether used internet from any location during last 30 days',\n",
              "       'No. of days stayed away from home during last 30 days',\n",
              "       'No. of meals usually taken in a day',\n",
              "       'No. of meals taken during last 30 days from school, balwadi etc.',\n",
              "       'No. of meals taken during last 30 days from employer as perquisites or part of wage',\n",
              "       'No. of meals taken during last 30 days  others',\n",
              "       'No. of meals taken during last 30 days on payment',\n",
              "       'No. of meals taken during last 30 days at home', 'Sector', 'State',\n",
              "       'NSS-Region', 'District', 'Household Type',\n",
              "       'Religion of the head of the household',\n",
              "       'Social Group of the head of the household', 'HH Size (For FDQ)',\n",
              "       'NCO_3D', 'NIC_5D', 'Is_online_Clothing_Purchased_Last365',\n",
              "       'Is_online_Footwear_Purchased_Last365',\n",
              "       'Is_online_Furniture_fixturesPurchased_Last365',\n",
              "       'Is_online_Mobile_Handset_Purchased_Last365',\n",
              "       'Is_online_Personal_Goods_Purchased_Last365',\n",
              "       'Is_online_Recreation_Goods_Purchased_Last365',\n",
              "       'Is_online_Household_Appliances_Purchased_Last365',\n",
              "       'Is_online_Crockery_Utensils_Purchased_Last365',\n",
              "       'Is_online_Sports_Goods_Purchased_Last365',\n",
              "       'Is_online_Medical_Equipment_Purchased_Last365',\n",
              "       'Is_online_Bedding_Purchased_Last365', 'Is_HH_Have_Television',\n",
              "       'Is_HH_Have_Radio', 'Is_HH_Have_Laptop_PC', 'Is_HH_Have_Mobile_handset',\n",
              "       'Is_HH_Have_Bicycle', 'Is_HH_Have_Motorcycle_scooter',\n",
              "       'Is_HH_Have_Motorcar_jeep_van', 'Is_HH_Have_Trucks',\n",
              "       'Is_HH_Have_Animal_cart', 'Is_HH_Have_Refrigerator',\n",
              "       'Is_HH_Have_Washing_machine', 'Is_HH_Have_Airconditioner_aircooler',\n",
              "       'TotalExpense'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_final.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "p0w9yMghDYp0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column we want to modify\n",
        "employer_meals_col = 'No. of meals taken during last 30 days from employer as perquisites or part of wage'\n",
        "household_type_col = 'Household Type'  # Adjust if your household type column has a different name\n",
        "\n",
        "# Create a mask for the specified household types (1, 2, 5, and 6)\n",
        "household_mask = train_data_final[household_type_col].isin([1, 2, 5, 6])\n",
        "household_mask_test = test_data_final[household_type_col].isin([1, 2, 5, 6])\n",
        "# Fill missing values with 0 for the specified household types\n",
        "train_data_final.loc[household_mask & train_data_final[employer_meals_col].isna(), employer_meals_col] = 0\n",
        "test_data_final.loc[household_mask_test & test_data_final[employer_meals_col].isna(), employer_meals_col] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "myyA0f8eEoBM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column we want to modify\n",
        "school_meals_col = 'No. of meals taken during last 30 days from school, balwadi etc.'\n",
        "age_col = 'Age(in years)'  # Adjust if your age column has a different name\n",
        "\n",
        "# Create a mask for individuals over 14 years old\n",
        "age_mask = train_data_final[age_col] > 14\n",
        "age_mask_test = test_data_final[age_col] > 14\n",
        "\n",
        "# Fill missing values with 0 for individuals over 14\n",
        "train_data_final.loc[age_mask & train_data_final[school_meals_col].isna(), school_meals_col] = 0\n",
        "test_data_final.loc[age_mask_test & test_data_final[school_meals_col].isna(), school_meals_col] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "smICQ1oyFuDf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column we want to modify\n",
        "payment_meals_col = 'No. of meals taken during last 30 days on payment'\n",
        "sector_col = 'Sector'  # Adjust if your sector column has a different name\n",
        "\n",
        "# Create a mask for sector 2\n",
        "sector_mask = train_data_final[sector_col] == 1\n",
        "\n",
        "# Fill missing values with 0 for sector 2\n",
        "train_data_final.loc[sector_mask & train_data_final[payment_meals_col].isna(), payment_meals_col] = 0\n",
        "test_data_final.loc[sector_mask & test_data_final[payment_meals_col].isna(), payment_meals_col] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2SjVKsPGk_-",
        "outputId": "1c453aba-1ed4-4cf8-a388-0f82413a19f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 20 feature columns for imputation\n",
            "Imputing categorical column: Whether used internet from any location during last 30 days with 32034 missing values\n",
            "  Unique values: [2. 1.]\n",
            "  Imputation complete for Whether used internet from any location during last 30 days\n",
            "Imputing numeric column: No. of days stayed away from home during last 30 days with 4617 missing values\n",
            "Imputing numeric column in test: No. of days stayed away from home during last 30 days with 1171 missing values\n",
            "  Imputed 1171 values in test set for No. of days stayed away from home during last 30 days\n",
            "  Imputation complete for No. of days stayed away from home during last 30 days\n",
            "Imputing numeric column: No. of meals usually taken in a day with 2385 missing values\n",
            "Imputing numeric column in test: No. of meals usually taken in a day with 614 missing values\n",
            "  Imputed 614 values in test set for No. of meals usually taken in a day\n",
            "  Imputation complete for No. of meals usually taken in a day\n",
            "Imputing numeric column: No. of meals taken during last 30 days from school, balwadi etc. with 111649 missing values\n",
            "Imputing numeric column in test: No. of meals taken during last 30 days from school, balwadi etc. with 28059 missing values\n",
            "  Imputed 28059 values in test set for No. of meals taken during last 30 days from school, balwadi etc.\n",
            "  Imputation complete for No. of meals taken during last 30 days from school, balwadi etc.\n",
            "Imputing numeric column: No. of meals taken during last 30 days from employer as perquisites or part of wage with 125219 missing values\n",
            "Imputing numeric column in test: No. of meals taken during last 30 days from employer as perquisites or part of wage with 31222 missing values\n",
            "  Imputed 31222 values in test set for No. of meals taken during last 30 days from employer as perquisites or part of wage\n",
            "  Imputation complete for No. of meals taken during last 30 days from employer as perquisites or part of wage\n",
            "Imputing numeric column: No. of meals taken during last 30 days  others with 556701 missing values\n",
            "Imputing numeric column in test: No. of meals taken during last 30 days  others with 139775 missing values\n",
            "  Imputed 139775 values in test set for No. of meals taken during last 30 days  others\n",
            "  Imputation complete for No. of meals taken during last 30 days  others\n",
            "Imputing numeric column: No. of meals taken during last 30 days on payment with 206743 missing values\n",
            "Imputing numeric column in test: No. of meals taken during last 30 days on payment with 0 missing values\n",
            "  No missing values in test set for No. of meals taken during last 30 days on payment or empty prediction set\n",
            "  Imputation complete for No. of meals taken during last 30 days on payment\n",
            "Imputing numeric column: No. of meals taken during last 30 days at home with 6708 missing values\n",
            "Imputing numeric column in test: No. of meals taken during last 30 days at home with 1704 missing values\n",
            "  Imputed 1704 values in test set for No. of meals taken during last 30 days at home\n",
            "  Imputation complete for No. of meals taken during last 30 days at home\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "import time\n",
        "\n",
        "# Define the columns to impute\n",
        "categorical_columns = [\n",
        "    'Whether used internet from any location during last 30 days'\n",
        "]\n",
        "\n",
        "numeric_columns = [\n",
        "    'No. of days stayed away from home during last 30 days',\n",
        "    'No. of meals usually taken in a day',\n",
        "    'No. of meals taken during last 30 days from school, balwadi etc.',\n",
        "    'No. of meals taken during last 30 days from employer as perquisites or part of wage',\n",
        "    'No. of meals taken during last 30 days  others',\n",
        "    'No. of meals taken during last 30 days on payment',\n",
        "    'No. of meals taken during last 30 days at home'\n",
        "]\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Verify columns exist\n",
        "categorical_exists = [col for col in categorical_columns if col in train_data_final.columns]\n",
        "categorical_exists_test = [col for col in categorical_columns if col in test_data_final.columns] \n",
        "numeric_exists = [col for col in numeric_columns if col in train_data_final.columns]\n",
        "numeric_exists_test = [col for col in numeric_columns if col in test_data_final.columns]\n",
        "\n",
        "all_columns_to_impute = categorical_exists + numeric_exists\n",
        "all_columns_to_impute_test = categorical_exists_test + numeric_exists_test\n",
        "\n",
        "# Identify feature columns for prediction (exclude columns we're imputing)\n",
        "feature_cols = [col for col in train_data_final.select_dtypes(include=['int64', 'float64']).columns\n",
        "               if col not in all_columns_to_impute]\n",
        "feature_cols_test = [col for col in test_data_final.select_dtypes(include=['int64', 'float64']).columns\n",
        "               if col not in all_columns_to_impute_test]\n",
        "\n",
        "# If we have too many features, we can limit to a subset\n",
        "if len(feature_cols) > 20:\n",
        "    feature_cols = feature_cols[:20]  # Using first 20 as an example\n",
        "    feature_cols_test = feature_cols_test[:20]\n",
        "\n",
        "print(f\"Using {len(feature_cols)} feature columns for imputation\")\n",
        "\n",
        "# First, impute the categorical column using RandomForestClassifier\n",
        "for col in categorical_exists:\n",
        "    missing_mask = train_data_final[col].isnull()\n",
        "    missing_mask_test = test_data_final[col].isnull()\n",
        "    missing_count = missing_mask.sum()\n",
        "    missing_count_test = missing_mask_test.sum()\n",
        "\n",
        "    if missing_count == 0:\n",
        "        print(f\"No missing values in {col}, skipping\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Imputing categorical column: {col} with {missing_count} missing values\")\n",
        "\n",
        "    # Check unique values to ensure it's categorical\n",
        "    unique_values = train_data_final[col].dropna().unique()\n",
        "    unique_values_test = test_data_final[col].dropna().unique()\n",
        "    print(f\"  Unique values: {unique_values}\")\n",
        "\n",
        "    # Split data into rows with the value (for training) and rows without (to predict)\n",
        "    train_data = train_data_final[~missing_mask]\n",
        "    test_data = test_data_final[~missing_mask_test]\n",
        "    predict_data = train_data_final[missing_mask]\n",
        "    predict_data_test = test_data_final[missing_mask_test]\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = train_data[feature_cols].fillna(-999)  # Simple handling for missing features\n",
        "    X_train_test = test_data[feature_cols_test].fillna(-999)\n",
        "    y_train = train_data[col]\n",
        "    y_train_test = test_data[col]\n",
        "\n",
        "    # Prepare prediction data\n",
        "    X_predict = predict_data[feature_cols].fillna(-999)\n",
        "    X_predict_test = predict_data_test[feature_cols_test].fillna(-999)\n",
        "\n",
        "    # Train RandomForestClassifier\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "    # Predict missing values\n",
        "    predictions = clf.predict(X_predict)\n",
        "    predictions_test = clf.predict(X_predict_test)\n",
        "\n",
        "    # Fill the missing values with predictions\n",
        "    train_data_final.loc[missing_mask, col] = predictions\n",
        "    test_data_final.loc[missing_mask_test, col] = predictions_test\n",
        "\n",
        "    print(f\"  Imputation complete for {col}\")\n",
        "\n",
        "# Next, impute the numeric columns using RandomForestRegressor\n",
        "# Modify the section in the numeric column imputation loop\n",
        "for col in numeric_exists:\n",
        "    missing_mask = train_data_final[col].isnull()\n",
        "    missing_mask_test = test_data_final[col].isnull()\n",
        "    missing_count = missing_mask.sum()\n",
        "    missing_count_test = missing_mask_test.sum()\n",
        "\n",
        "    if missing_count == 0:\n",
        "        print(f\"No missing values in {col}, skipping\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Imputing numeric column: {col} with {missing_count} missing values\")\n",
        "    print(f\"Imputing numeric column in test: {col} with {missing_count_test} missing values\")\n",
        "\n",
        "    # Split data into rows with the value (for training) and rows without (to predict)\n",
        "    train_data = train_data_final[~missing_mask]\n",
        "    test_data = test_data_final[~missing_mask_test]\n",
        "    predict_data = train_data_final[missing_mask]\n",
        "    predict_data_test = test_data_final[missing_mask_test]\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = train_data[feature_cols].fillna(-999)\n",
        "    y_train = train_data[col]\n",
        "\n",
        "    # Prepare prediction data\n",
        "    X_predict = predict_data[feature_cols].fillna(-999)\n",
        "\n",
        "    # Train RandomForestRegressor\n",
        "    regr = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    regr.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict missing values - train data\n",
        "    predictions = regr.predict(X_predict)\n",
        "    \n",
        "    # Round to nearest integer since these are count variables\n",
        "    predictions = np.round(predictions).astype(int)\n",
        "    # Ensure predictions are non-negative\n",
        "    predictions = np.maximum(0, predictions)\n",
        "    # Fill the missing values with predictions\n",
        "    train_data_final.loc[missing_mask, col] = predictions\n",
        "    \n",
        "    # Only predict for test data if there are missing values\n",
        "    if missing_count_test > 0 and len(predict_data_test) > 0:\n",
        "        # Use the same model for test data\n",
        "        X_predict_test = predict_data_test[feature_cols_test].fillna(-999)\n",
        "        predictions_test = regr.predict(X_predict_test)\n",
        "        predictions_test = np.round(predictions_test).astype(int)\n",
        "        predictions_test = np.maximum(0, predictions_test)\n",
        "        test_data_final.loc[missing_mask_test, col] = predictions_test\n",
        "        print(f\"  Imputed {missing_count_test} values in test set for {col}\")\n",
        "    else:\n",
        "        print(f\"  No missing values in test set for {col} or empty prediction set\")\n",
        "    \n",
        "    print(f\"  Imputation complete for {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2a9ogaMQH3PW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of                                        HH_ID  Person Srl No.  \\\n",
            "0        HCES2022655561010131113011101202201               1   \n",
            "1        HCES2022655561010131113011101202201               2   \n",
            "2        HCES2022655561010131113011101202201               3   \n",
            "3        HCES2022655561010131113011101202201               4   \n",
            "4        HCES2022655561010131113011101202201               5   \n",
            "...                                      ...             ...   \n",
            "901718  HCES20223492023737102120110201201313               1   \n",
            "901719  HCES20223492023737102120110201201313               2   \n",
            "901720  HCES20223492023737102120110201201313               3   \n",
            "901721  HCES20223492023737102120110201201313               4   \n",
            "901722  HCES20223492023737102120110201201313               5   \n",
            "\n",
            "        Relation to head (code)  Gender  Age(in years)  Marital Status (code)  \\\n",
            "0                             1       1             48                      2   \n",
            "1                             2       2             46                      2   \n",
            "2                             5       1             24                      1   \n",
            "3                             5       1             18                      1   \n",
            "4                             5       2             21                      1   \n",
            "...                         ...     ...            ...                    ...   \n",
            "901718                        1       1             40                      2   \n",
            "901719                        2       2             38                      2   \n",
            "901720                        5       1             12                      1   \n",
            "901721                        5       2              7                      1   \n",
            "901722                        5       2              5                      1   \n",
            "\n",
            "        Highest educational level attained (code)  \\\n",
            "0                                               6   \n",
            "1                                               1   \n",
            "2                                              13   \n",
            "3                                               7   \n",
            "4                                              12   \n",
            "...                                           ...   \n",
            "901718                                         12   \n",
            "901719                                          6   \n",
            "901720                                          4   \n",
            "901721                                          3   \n",
            "901722                                          3   \n",
            "\n",
            "        Total year of education completed  \\\n",
            "0                                    12.0   \n",
            "1                                     0.0   \n",
            "2                                    18.0   \n",
            "3                                    13.0   \n",
            "4                                    17.0   \n",
            "...                                   ...   \n",
            "901718                               17.0   \n",
            "901719                               12.0   \n",
            "901720                                7.0   \n",
            "901721                                2.0   \n",
            "901722                                3.0   \n",
            "\n",
            "        Whether used internet from any location during last 30 days  \\\n",
            "0                                                     2.0             \n",
            "1                                                     2.0             \n",
            "2                                                     1.0             \n",
            "3                                                     1.0             \n",
            "4                                                     2.0             \n",
            "...                                                   ...             \n",
            "901718                                                1.0             \n",
            "901719                                                1.0             \n",
            "901720                                                1.0             \n",
            "901721                                                1.0             \n",
            "901722                                                1.0             \n",
            "\n",
            "        No. of days stayed away from home during last 30 days  ...  \\\n",
            "0                                                     0.0      ...   \n",
            "1                                                     0.0      ...   \n",
            "2                                                     0.0      ...   \n",
            "3                                                     0.0      ...   \n",
            "4                                                     0.0      ...   \n",
            "...                                                   ...      ...   \n",
            "901718                                                0.0      ...   \n",
            "901719                                                0.0      ...   \n",
            "901720                                                0.0      ...   \n",
            "901721                                                0.0      ...   \n",
            "901722                                                0.0      ...   \n",
            "\n",
            "        Is_HH_Have_Mobile_handset  Is_HH_Have_Bicycle  \\\n",
            "0                               1                   1   \n",
            "1                               1                   1   \n",
            "2                               1                   1   \n",
            "3                               1                   1   \n",
            "4                               1                   1   \n",
            "...                           ...                 ...   \n",
            "901718                          1                   0   \n",
            "901719                          1                   0   \n",
            "901720                          1                   0   \n",
            "901721                          1                   0   \n",
            "901722                          1                   0   \n",
            "\n",
            "        Is_HH_Have_Motorcycle_scooter  Is_HH_Have_Motorcar_jeep_van  \\\n",
            "0                                   1                             1   \n",
            "1                                   1                             1   \n",
            "2                                   1                             1   \n",
            "3                                   1                             1   \n",
            "4                                   1                             1   \n",
            "...                               ...                           ...   \n",
            "901718                              0                             0   \n",
            "901719                              0                             0   \n",
            "901720                              0                             0   \n",
            "901721                              0                             0   \n",
            "901722                              0                             0   \n",
            "\n",
            "        Is_HH_Have_Trucks  Is_HH_Have_Animal_cart  Is_HH_Have_Refrigerator  \\\n",
            "0                       0                       0                        1   \n",
            "1                       0                       0                        1   \n",
            "2                       0                       0                        1   \n",
            "3                       0                       0                        1   \n",
            "4                       0                       0                        1   \n",
            "...                   ...                     ...                      ...   \n",
            "901718                  0                       0                        1   \n",
            "901719                  0                       0                        1   \n",
            "901720                  0                       0                        1   \n",
            "901721                  0                       0                        1   \n",
            "901722                  0                       0                        1   \n",
            "\n",
            "        Is_HH_Have_Washing_machine  Is_HH_Have_Airconditioner_aircooler  \\\n",
            "0                                1                                    0   \n",
            "1                                1                                    0   \n",
            "2                                1                                    0   \n",
            "3                                1                                    0   \n",
            "4                                1                                    0   \n",
            "...                            ...                                  ...   \n",
            "901718                           1                                    0   \n",
            "901719                           1                                    0   \n",
            "901720                           1                                    0   \n",
            "901721                           1                                    0   \n",
            "901722                           1                                    0   \n",
            "\n",
            "        TotalExpense  \n",
            "0       30903.434442  \n",
            "1       30903.434442  \n",
            "2       30903.434442  \n",
            "3       30903.434442  \n",
            "4       30903.434442  \n",
            "...              ...  \n",
            "901718  25570.880626  \n",
            "901719  25570.880626  \n",
            "901720  25570.880626  \n",
            "901721  25570.880626  \n",
            "901722  25570.880626  \n",
            "\n",
            "[901723 rows x 50 columns]>\n"
          ]
        }
      ],
      "source": [
        "print(train_data_final.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VseCSGyVKusm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing Values in Merged Train Data:\n",
            "HH_ID                                                                                      0\n",
            "Person Srl No.                                                                             0\n",
            "Relation to head (code)                                                                    0\n",
            "Gender                                                                                     0\n",
            "Age(in years)                                                                              0\n",
            "Marital Status (code)                                                                      0\n",
            "Highest educational level attained (code)                                                  0\n",
            "Total year of education completed                                                          0\n",
            "Whether used internet from any location during last 30 days                                0\n",
            "No. of days stayed away from home during last 30 days                                      0\n",
            "No. of meals usually taken in a day                                                        0\n",
            "No. of meals taken during last 30 days from school, balwadi etc.                           0\n",
            "No. of meals taken during last 30 days from employer as perquisites or part of wage        0\n",
            "No. of meals taken during last 30 days  others                                             0\n",
            "No. of meals taken during last 30 days on payment                                          0\n",
            "No. of meals taken during last 30 days at home                                             0\n",
            "Sector                                                                                     0\n",
            "State                                                                                      0\n",
            "NSS-Region                                                                                 0\n",
            "District                                                                                   0\n",
            "Household Type                                                                             0\n",
            "Religion of the head of the household                                                      0\n",
            "Social Group of the head of the household                                                  0\n",
            "HH Size (For FDQ)                                                                          0\n",
            "NCO_3D                                                                                 12215\n",
            "NIC_5D                                                                                 12215\n",
            "Is_online_Clothing_Purchased_Last365                                                       0\n",
            "Is_online_Footwear_Purchased_Last365                                                       0\n",
            "Is_online_Furniture_fixturesPurchased_Last365                                              0\n",
            "Is_online_Mobile_Handset_Purchased_Last365                                                 0\n",
            "Is_online_Personal_Goods_Purchased_Last365                                                 0\n",
            "Is_online_Recreation_Goods_Purchased_Last365                                               0\n",
            "Is_online_Household_Appliances_Purchased_Last365                                           0\n",
            "Is_online_Crockery_Utensils_Purchased_Last365                                              0\n",
            "Is_online_Sports_Goods_Purchased_Last365                                                   0\n",
            "Is_online_Medical_Equipment_Purchased_Last365                                              0\n",
            "Is_online_Bedding_Purchased_Last365                                                        0\n",
            "Is_HH_Have_Television                                                                      0\n",
            "Is_HH_Have_Radio                                                                           0\n",
            "Is_HH_Have_Laptop_PC                                                                       0\n",
            "Is_HH_Have_Mobile_handset                                                                  0\n",
            "Is_HH_Have_Bicycle                                                                         0\n",
            "Is_HH_Have_Motorcycle_scooter                                                              0\n",
            "Is_HH_Have_Motorcar_jeep_van                                                               0\n",
            "Is_HH_Have_Trucks                                                                          0\n",
            "Is_HH_Have_Animal_cart                                                                     0\n",
            "Is_HH_Have_Refrigerator                                                                    0\n",
            "Is_HH_Have_Washing_machine                                                                 0\n",
            "Is_HH_Have_Airconditioner_aircooler                                                        0\n",
            "TotalExpense                                                                               0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMissing Values in Merged Train Data:\")\n",
        "#print(train_data_final.isnull().sum())\n",
        "print(test_data_final.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_final.to_csv('Merged_train_data_final.csv', index=False)\n",
        "test_data_final.to_csv('Merged_test_data_filnal.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "X6A3OVEvuvVS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column names in updated dataframe: ['HH_ID', 'Person Srl No.', 'Relation to head (code)', 'Gender', 'Age(in years)', 'Marital Status (code)', 'Highest educational level attained (code)', 'Total year of education completed', 'Whether used internet from any location during last 30 days', 'No. of days stayed away from home during last 30 days', 'No. of meals usually taken in a day', 'No. of meals taken during last 30 days from school, balwadi etc.', 'No. of meals taken during last 30 days from employer as perquisites or part of wage', 'No. of meals taken during last 30 days  others', 'No. of meals taken during last 30 days on payment', 'No. of meals taken during last 30 days at home', 'Sector', 'State', 'NSS-Region', 'District', 'Household Type', 'Religion of the head of the household', 'Social Group of the head of the household', 'HH Size (For FDQ)', 'NCO_3D', 'NIC_5D', 'Is_online_Clothing_Purchased_Last365', 'Is_online_Footwear_Purchased_Last365', 'Is_online_Furniture_fixturesPurchased_Last365', 'Is_online_Mobile_Handset_Purchased_Last365', 'Is_online_Personal_Goods_Purchased_Last365', 'Is_online_Recreation_Goods_Purchased_Last365', 'Is_online_Household_Appliances_Purchased_Last365', 'Is_online_Crockery_Utensils_Purchased_Last365', 'Is_online_Sports_Goods_Purchased_Last365', 'Is_online_Medical_Equipment_Purchased_Last365', 'Is_online_Bedding_Purchased_Last365', 'Is_HH_Have_Television', 'Is_HH_Have_Radio', 'Is_HH_Have_Laptop_PC', 'Is_HH_Have_Mobile_handset', 'Is_HH_Have_Bicycle', 'Is_HH_Have_Motorcycle_scooter', 'Is_HH_Have_Motorcar_jeep_van', 'Is_HH_Have_Trucks', 'Is_HH_Have_Animal_cart', 'Is_HH_Have_Refrigerator', 'Is_HH_Have_Washing_machine', 'Is_HH_Have_Airconditioner_aircooler', 'TotalExpense', 'WeightedExpense']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ww/qjh9zpks52d4vsjxc4bg379h0000gn/T/ipykernel_72684/2085299322.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  result = train_data_final.groupby('HH_ID').apply(calculate_weighted_expenses)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate weighted expenses for each individual in a family\n",
        "def calculate_weighted_expenses(group):\n",
        "    # Sort adults first (descending by age) so we can assign weights correctly\n",
        "    group = group.sort_values(by='Age(in years)', ascending=False)\n",
        "\n",
        "    # Create weight column based on age\n",
        "    weights = []\n",
        "    adult_count = 0\n",
        "\n",
        "    for age in group['Age(in years)']:\n",
        "        if age > 18:  # Adult\n",
        "            adult_count += 1\n",
        "            if adult_count == 1:\n",
        "                weights.append(1.0)  # First adult\n",
        "            else:\n",
        "                weights.append(0.7)  # Subsequent adults\n",
        "        else:  # Child\n",
        "            weights.append(0.5)\n",
        "\n",
        "    # Add temporary weight column to the group\n",
        "    group['temp_weight'] = weights\n",
        "\n",
        "    # Calculate sum of weights for the family\n",
        "    total_weight = sum(weights)\n",
        "\n",
        "    # Calculate weighted expense for each individual\n",
        "    # Formula: TotalExpense × (individual weight / sum of weights)\n",
        "    total_expense = group['TotalExpense'].iloc[0]  # Assuming TotalExpense is the same for all family members\n",
        "\n",
        "    # Calculate weighted expense for each individual based on their proportion of the total weight\n",
        "    group['WeightedExpense'] = group['temp_weight'].apply(lambda x: total_expense * (x / total_weight))\n",
        "\n",
        "    # Drop the temporary weight column\n",
        "    group = group.drop(columns=['temp_weight'])\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply the function to create a new dataframe with the calculations\n",
        "result = train_data_final.groupby('HH_ID').apply(calculate_weighted_expenses)\n",
        "\n",
        "# Reset the index to get a clean dataframe\n",
        "if isinstance(result.index, pd.MultiIndex):\n",
        "    result = result.reset_index(drop=True)\n",
        "\n",
        "# Now replace the original dataframe with the updated one\n",
        "train_data_final = result\n",
        "\n",
        "# The train_data_final dataframe now includes the WeightedExpense column\n",
        "\n",
        "# Optional: If you want to verify the changes\n",
        "print(\"Column names in updated dataframe:\", train_data_final.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values after filling:\n",
            "NCO_3D    0\n",
            "NIC_5D    0\n",
            "dtype: int64\n",
            "Data saved to 'train_data_final.csv'\n"
          ]
        }
      ],
      "source": [
        "# Fill missing values in specific columns with 1000\n",
        "train_data_final[['NCO_3D', 'NIC_5D']] = train_data_final[['NCO_3D', 'NIC_5D']].fillna(1000)\n",
        "test_data_final[['NCO_3D', 'NIC_5D']] = test_data_final[['NCO_3D', 'NIC_5D']].fillna(1000)\n",
        "\n",
        "# Drop 'TotalExpense' column if it exists\n",
        "train_data_final = train_data_final.drop(columns=['TotalExpense'], errors='ignore')\n",
        "# Confirm that there are no more missing values in those columns\n",
        "print(\"Missing values after filling:\")\n",
        "print(train_data_final[['NCO_3D', 'NIC_5D']].isnull().sum())\n",
        "\n",
        "# Save to CSV\n",
        "train_data_final.to_csv(\"train_data_final.csv\", index=False)\n",
        "test_data_final.to_csv(\"test_data_final.csv\", index=False)\n",
        "print(\"Data saved to 'train_data_final.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# # Standardize numeric features, exclude non-numeric (e.g., IDs)\n",
        "# def standardize_features(df, target_column):\n",
        "#     non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
        "#     if target_column in non_numeric_cols:\n",
        "#         non_numeric_cols.remove(target_column)\n",
        "\n",
        "#     X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
        "#     y = df[target_column].values\n",
        "#     scaler = StandardScaler()\n",
        "#     X_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "#     return X_scaled, y, scaler, non_numeric_cols\n",
        "\n",
        "# # Elbow method to find optimal clusters\n",
        "# def elbow_method_auto(X, max_clusters=10):\n",
        "#     inertias = []\n",
        "#     for k in range(1, max_clusters + 1):\n",
        "#         kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "#         kmeans.fit(X)\n",
        "#         inertias.append(kmeans.inertia_)\n",
        "#     # Find elbow: largest drop in inertia\n",
        "#     deltas = np.diff(inertias)\n",
        "#     second_deltas = np.diff(deltas)\n",
        "#     elbow_point = np.argmin(second_deltas) + 2  # +2 accounts for double diff index shift\n",
        "#     return elbow_point\n",
        "\n",
        "# # Mean Percentage Error\n",
        "# def mean_percentage_error(y_true, y_pred):\n",
        "#     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# # Evaluate and print metrics\n",
        "# def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
        "#     mpe = mean_percentage_error(y_true, y_pred)\n",
        "#     mae = mean_absolute_error(y_true, y_pred)\n",
        "#     r2 = r2_score(y_true, y_pred)\n",
        "#     print(f\"\\n--- {method_name} ---\")\n",
        "#     print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
        "#     print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "#     print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# # Main function: cluster, predict, evaluate\n",
        "# def cluster_and_predict(train_data_final, target_column='WeightedExpense', max_clusters=10):\n",
        "#     # Step 1: Standardize numeric features\n",
        "#     X_scaled, y, scaler, non_numeric_cols = standardize_features(train_data_final, target_column)\n",
        "\n",
        "#     # Step 2: Find optimal clusters\n",
        "#     optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
        "#     print(f\"Optimal number of clusters (Elbow method): {optimal_k}\")\n",
        "\n",
        "#     # Step 3: KMeans clustering\n",
        "#     kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
        "#     clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "#     # Add cluster labels back to DataFrame\n",
        "#     df_with_clusters = train_data_final.copy()\n",
        "#     df_with_clusters['Cluster'] = clusters\n",
        "\n",
        "#     # Step 4A: Cluster-wise regression\n",
        "#     preds_regression = np.zeros_like(y)\n",
        "#     for cluster_id in np.unique(clusters):\n",
        "#         cluster_idx = clusters == cluster_id\n",
        "#         X_cluster = X_scaled[cluster_idx]\n",
        "#         y_cluster = y[cluster_idx]\n",
        "\n",
        "#         # Train regression model on cluster\n",
        "#         model = LinearRegression()\n",
        "#         model.fit(X_cluster, y_cluster)\n",
        "#         preds_regression[cluster_idx] = model.predict(X_cluster)\n",
        "\n",
        "#     evaluate_predictions(y, preds_regression, method_name=\"Cluster-wise Regression\")\n",
        "\n",
        "#     # Step 4B: Cluster mean assignment\n",
        "#     preds_mean = np.zeros_like(y)\n",
        "#     for cluster_id in np.unique(clusters):\n",
        "#         cluster_idx = clusters == cluster_id\n",
        "#         y_cluster = y[cluster_idx]\n",
        "#         cluster_mean = np.mean(y_cluster)\n",
        "#         preds_mean[cluster_idx] = cluster_mean\n",
        "\n",
        "#     evaluate_predictions(y, preds_mean, method_name=\"Cluster Mean Assignment\")\n",
        "\n",
        "#     return preds_regression, preds_mean, clusters, optimal_k\n",
        "\n",
        "# # Run the entire pipeline\n",
        "# preds_reg, preds_mean, clusters, optimal_k = cluster_and_predict(train_data_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import mean_absolute_error, r2_score\n",
        "# from xgboost import XGBRegressor\n",
        "\n",
        "# # Standardize numeric features, exclude non-numeric (e.g., IDs)\n",
        "# def standardize_features(df, target_column):\n",
        "#     non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
        "#     if target_column in non_numeric_cols:\n",
        "#         non_numeric_cols.remove(target_column)\n",
        "\n",
        "#     X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
        "#     y = df[target_column].values\n",
        "#     scaler = StandardScaler()\n",
        "#     X_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "#     return X_scaled, y, scaler, non_numeric_cols\n",
        "\n",
        "# # Elbow method to find optimal clusters\n",
        "# def elbow_method_auto(X, max_clusters=10):\n",
        "#     inertias = []\n",
        "#     for k in range(1, max_clusters + 1):\n",
        "#         kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "#         kmeans.fit(X)\n",
        "#         inertias.append(kmeans.inertia_)\n",
        "#     # Find elbow: largest drop in inertia\n",
        "#     deltas = np.diff(inertias)\n",
        "#     second_deltas = np.diff(deltas)\n",
        "#     elbow_point = np.argmin(second_deltas) + 2  # +2 accounts for double diff index shift\n",
        "#     return elbow_point\n",
        "\n",
        "# # Mean Percentage Error\n",
        "# def mean_percentage_error(y_true, y_pred):\n",
        "#     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# # Evaluate and print metrics\n",
        "# def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
        "#     mpe = mean_percentage_error(y_true, y_pred)\n",
        "#     mae = mean_absolute_error(y_true, y_pred)\n",
        "#     r2 = r2_score(y_true, y_pred)\n",
        "#     print(f\"\\n--- {method_name} ---\")\n",
        "#     print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
        "#     print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "#     print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# # Select model based on user input\n",
        "# def get_model(model_type):\n",
        "#     if model_type == 'linear':\n",
        "#         return LinearRegression()\n",
        "#     elif model_type == 'random_forest':\n",
        "#         return RandomForestRegressor(random_state=42, n_estimators=100)\n",
        "#     elif model_type == 'xgboost':\n",
        "#         return XGBRegressor(random_state=42, n_estimators=100, verbosity=0)\n",
        "#     else:\n",
        "#         raise ValueError(\"Invalid model_type. Choose from 'linear', 'random_forest', or 'xgboost'.\")\n",
        "\n",
        "# # Main function: cluster, predict, evaluate\n",
        "# def cluster_and_predict(train_data_final, target_column='WeightedExpense', max_clusters=10, model_type='linear'):\n",
        "#     # Step 1: Standardize numeric features\n",
        "#     X_scaled, y, scaler, non_numeric_cols = standardize_features(train_data_final, target_column)\n",
        "\n",
        "#     # Step 2: Find optimal clusters\n",
        "#     optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
        "#     print(f\"Optimal number of clusters (Elbow method): {optimal_k}\")\n",
        "\n",
        "#     # Step 3: KMeans clustering\n",
        "#     kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
        "#     clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "#     # Add cluster labels back to DataFrame\n",
        "#     df_with_clusters = train_data_final.copy()\n",
        "#     df_with_clusters['Cluster'] = clusters\n",
        "\n",
        "#     # Step 4A: Cluster-wise model prediction (Flexible)\n",
        "#     preds_model = np.zeros_like(y)\n",
        "#     for cluster_id in np.unique(clusters):\n",
        "#         cluster_idx = clusters == cluster_id\n",
        "#         X_cluster = X_scaled[cluster_idx]\n",
        "#         y_cluster = y[cluster_idx]\n",
        "\n",
        "#         model = get_model(model_type)\n",
        "#         model.fit(X_cluster, y_cluster)\n",
        "#         preds_model[cluster_idx] = model.predict(X_cluster)\n",
        "\n",
        "#     evaluate_predictions(y, preds_model, method_name=f\"Cluster-wise {model_type.capitalize()} Prediction\")\n",
        "\n",
        "#     # Step 4B: Cluster mean assignment\n",
        "#     preds_mean = np.zeros_like(y)\n",
        "#     for cluster_id in np.unique(clusters):\n",
        "#         cluster_idx = clusters == cluster_id\n",
        "#         y_cluster = y[cluster_idx]\n",
        "#         cluster_mean = np.mean(y_cluster)\n",
        "#         preds_mean[cluster_idx] = cluster_mean\n",
        "\n",
        "#     evaluate_predictions(y, preds_mean, method_name=\"Cluster Mean Assignment\")\n",
        "\n",
        "#     return preds_model, preds_mean, clusters, optimal_k\n",
        "\n",
        "# # Example usage (Replace with your actual DataFrame)\n",
        "# preds_model, preds_mean, clusters, optimal_k = cluster_and_predict(train_data_final, model_type='random_forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --- Standardize numeric features ---\n",
        "def standardize_features(file_path, target_column):\n",
        "    df = pd.read_csv(file_path)\n",
        "    non_numeric_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()\n",
        "    if target_column in non_numeric_cols:\n",
        "        non_numeric_cols.remove(target_column)\n",
        "    X_numeric = df.drop(columns=[target_column] + non_numeric_cols)\n",
        "    y = df[target_column].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_numeric)\n",
        "    return X_scaled, y, scaler, non_numeric_cols, X_numeric.columns.tolist()\n",
        "\n",
        "# --- Elbow method to determine optimal clusters ---\n",
        "def elbow_method_auto(X, max_clusters=10):\n",
        "    inertias = []\n",
        "    for k in range(1, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        kmeans.fit(X)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "    deltas = np.diff(inertias)\n",
        "    second_deltas = np.diff(deltas)\n",
        "    elbow_point = np.argmin(second_deltas) + 2  # Adjust for index shift\n",
        "    return elbow_point\n",
        "\n",
        "# --- Evaluation metrics ---\n",
        "def mean_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred, method_name=\"\"):\n",
        "    mpe = mean_percentage_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"\\n--- {method_name} ---\")\n",
        "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# --- Model selector ---\n",
        "def get_model(model_type):\n",
        "    if model_type == 'linear':\n",
        "        return LinearRegression()\n",
        "    elif model_type == 'random_forest':\n",
        "        return RandomForestRegressor(random_state=42, n_estimators=100)\n",
        "    elif model_type == 'xgboost':\n",
        "        return XGBRegressor(random_state=42, n_estimators=100, verbosity=0)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_type. Choose from 'linear', 'random_forest', or 'xgboost'.\")\n",
        "\n",
        "# --- Main Training Function ---\n",
        "def cluster_and_predict(file_path, target_column='WeightedExpense', max_clusters=10, model_type='random_forest'):\n",
        "    # Standardize\n",
        "    X_scaled, y, scaler, non_numeric_cols, numeric_cols = standardize_features(file_path, target_column)\n",
        "\n",
        "    # Determine optimal number of clusters\n",
        "    optimal_k = elbow_method_auto(X_scaled, max_clusters=max_clusters)\n",
        "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
        "\n",
        "    # KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
        "    clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    # Cluster-wise model training\n",
        "    preds_model = np.zeros_like(y)\n",
        "    cluster_models = {}\n",
        "\n",
        "    for cluster_id in np.unique(clusters):\n",
        "        cluster_idx = clusters == cluster_id\n",
        "        X_cluster = X_scaled[cluster_idx]\n",
        "        y_cluster = y[cluster_idx]\n",
        "\n",
        "        model = get_model(model_type)\n",
        "        model.fit(X_cluster, y_cluster)\n",
        "        preds_model[cluster_idx] = model.predict(X_cluster)\n",
        "\n",
        "        cluster_models[cluster_id] = model  # Save model per cluster\n",
        "\n",
        "    # Evaluate training performance\n",
        "    evaluate_predictions(y, preds_model, method_name=f\"Cluster-wise {model_type.capitalize()} Prediction\")\n",
        "\n",
        "    # Return trained components for prediction\n",
        "    return scaler, kmeans, cluster_models, numeric_cols, non_numeric_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_on_test_data(test_csv_path, scaler, kmeans, cluster_models, numeric_cols, non_numeric_cols, target_column='WeightedExpense'):\n",
        "    \"\"\"\n",
        "    Predict expenses on test data using trained models.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    test_csv_path : str\n",
        "        Path to the test CSV file\n",
        "    scaler : StandardScaler\n",
        "        Fitted scaler from training data\n",
        "    kmeans : KMeans\n",
        "        Trained KMeans model\n",
        "    cluster_models : dict\n",
        "        Dictionary of regression models for each cluster\n",
        "    numeric_cols : list\n",
        "        List of numeric column names used in training\n",
        "    non_numeric_cols : list\n",
        "        List of non-numeric column names from training\n",
        "    target_column : str\n",
        "        Name of the target column\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Test data with predictions\n",
        "    \"\"\"\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(test_csv_path)\n",
        "    \n",
        "    # Drop 'TotalExpense' if present\n",
        "    if 'TotalExpense' in test_data.columns:\n",
        "        test_data = test_data.drop(columns=['TotalExpense'])\n",
        "    \n",
        "    # Check if all required numeric columns exist in test data\n",
        "    missing_cols = [col for col in numeric_cols if col not in test_data.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Test data is missing columns that were used in training: {missing_cols}\")\n",
        "    \n",
        "    # Create a DataFrame with only the numeric columns in the correct order\n",
        "    X_numeric_test = pd.DataFrame(index=test_data.index)\n",
        "    for col in numeric_cols:\n",
        "        X_numeric_test[col] = test_data[col]\n",
        "    \n",
        "    # Verify no missing values\n",
        "    if X_numeric_test.isna().any().any():\n",
        "        cols_with_nan = X_numeric_test.columns[X_numeric_test.isna().any()].tolist()\n",
        "        print(f\"Warning: NaN values found in columns: {cols_with_nan}\")\n",
        "        \n",
        "        # Fill NaN values with appropriate defaults for each column\n",
        "        for col in cols_with_nan:\n",
        "            print(f\"Filling NaN values in {col}\")\n",
        "            if col in ['NCO_3D', 'NIC_5D']:\n",
        "                X_numeric_test[col] = X_numeric_test[col].fillna(1000)\n",
        "            else:\n",
        "                X_numeric_test[col] = X_numeric_test[col].fillna(X_numeric_test[col].median())\n",
        "    \n",
        "    # Double-check for any remaining NaN values\n",
        "    if X_numeric_test.isna().any().any():\n",
        "        raise ValueError(\"Failed to handle all NaN values in numeric features\")\n",
        "        \n",
        "    # Standardize features using the same scaler from training\n",
        "    X_scaled_test = scaler.transform(X_numeric_test)\n",
        "    \n",
        "    # Predict clusters\n",
        "    test_clusters = kmeans.predict(X_scaled_test)\n",
        "    \n",
        "    # Predict using cluster-specific models\n",
        "    predictions = np.zeros(X_scaled_test.shape[0])\n",
        "    \n",
        "    for cluster_id in np.unique(test_clusters):\n",
        "        cluster_idx = test_clusters == cluster_id\n",
        "        X_cluster_test = X_scaled_test[cluster_idx]\n",
        "        \n",
        "        if cluster_id in cluster_models:\n",
        "            model = cluster_models[cluster_id]\n",
        "            predictions[cluster_idx] = model.predict(X_cluster_test)\n",
        "        else:\n",
        "            print(f\"Warning: No model for cluster {cluster_id}, using average prediction\")\n",
        "            # Use average of all predictions for this cluster\n",
        "            cluster_means = np.mean([m.predict(X_cluster_test).mean() \n",
        "                                     for m in cluster_models.values()])\n",
        "            predictions[cluster_idx] = cluster_means\n",
        "    \n",
        "    # Add predictions to test DataFrame\n",
        "    test_data['predicted_expense'] = predictions\n",
        "    \n",
        "    # Save predictions to CSV\n",
        "    output_path = 'test_data_with_predictions.csv'\n",
        "    test_data.to_csv(output_path, index=False)\n",
        "    print(f\"\\nPredictions saved to {output_path}\")\n",
        "    \n",
        "    # Display sample predictions\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    print(test_data[['predicted_expense']].head())\n",
        "    \n",
        "    return test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal number of clusters: 7\n",
            "\n",
            "--- Cluster-wise Random_forest Prediction ---\n",
            "Mean Percentage Error (MPE): 8.61%\n",
            "Mean Absolute Error (MAE): 416.76\n",
            "R² Score: 0.9460\n",
            "\n",
            "Predictions saved to test_data_with_predictions.csv\n",
            "\n",
            "Sample Predictions:\n",
            "   predicted_expense\n",
            "0        6152.236371\n",
            "1        4721.460102\n",
            "2        4852.511551\n",
            "3        3360.630067\n",
            "4       10390.225895\n"
          ]
        }
      ],
      "source": [
        "# ✅ Training on DataFrame (NO CSV)\n",
        "scaler, kmeans, cluster_models, numeric_cols, non_numeric_cols = cluster_and_predict(\n",
        "   '/Users/rishav/Downloads/IIT_GN/Train_Data/train_data_final.csv',  # Your DataFrame\n",
        "    target_column='WeightedExpense',\n",
        "    model_type='random_forest'\n",
        ")\n",
        "\n",
        "# ✅ Predict on Test CSV (predicted_expense only)\n",
        "test_results = predict_on_test_data(\n",
        "    test_csv_path='/Users/rishav/Downloads/IIT_GN/Train_Data/test_data_final.csv',\n",
        "    scaler=scaler,\n",
        "    kmeans=kmeans,\n",
        "    cluster_models=cluster_models,\n",
        "    numeric_cols=numeric_cols,\n",
        "    non_numeric_cols=non_numeric_cols,\n",
        "    target_column='WeightedExpense'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved data with household sums to test_data_with_household_sums.csv\n",
            "\n",
            "Sample rows from household HCES2022655561010131113011101202304:\n",
            "                                 HH_ID  predicted_expense\n",
            "0  HCES2022655561010131113011101202304       19086.838091\n",
            "1  HCES2022655561010131113011101202304       19086.838091\n",
            "2  HCES2022655561010131113011101202304       19086.838091\n",
            "3  HCES2022655561010131113011101202304       19086.838091\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Calculate the sum of predicted_expense for each household ID\n",
        "household_sums = test_results.groupby('HH_ID')['predicted_expense'].sum()\n",
        "\n",
        "# Step 2: Replace each row's predicted_expense with its household sum\n",
        "test_results['predicted_expense'] = test_results['HH_ID'].map(household_sums)\n",
        "\n",
        "# Step 3: Save the updated dataframe to CSV\n",
        "output_path = 'test_data_with_household_sums.csv'\n",
        "test_results.to_csv(output_path, index=False)\n",
        "print(f\"Saved data with household sums to {output_path}\")\n",
        "\n",
        "# Step 4: Verify the result with a sample household\n",
        "# (This step is optional but helps confirm the change worked correctly)\n",
        "sample_household = test_results['HH_ID'].iloc[0]\n",
        "sample_rows = test_results[test_results['HH_ID'] == sample_household].head()\n",
        "print(f\"\\nSample rows from household {sample_household}:\")\n",
        "print(sample_rows[['HH_ID', 'predicted_expense']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of households for comparison: 52350\n",
            "\n",
            "----- Evaluation Metrics -----\n",
            "R² Score: 0.5382\n",
            "Mean Squared Error (MSE): 85957334.25\n",
            "Root Mean Squared Error (RMSE): 9271.32\n",
            "Mean Absolute Error (MAE): 5364.99\n",
            "Mean Percentage Error (MPE): 28.18%\n",
            "Mean Absolute Percentage Error (MAPE): 28.18%\n",
            "\n",
            "----- Error Distribution -----\n",
            "Min Error: -391799.57\n",
            "Max Error: 127412.90\n",
            "Mean Error: 247.09\n",
            "Median Error: 1046.74\n",
            "\n",
            "----- Percentage Error Quantiles -----\n",
            "10th percentile: -27.64%\n",
            "25th percentile: -12.25%\n",
            "50th percentile: 6.93%\n",
            "75th percentile: 30.49%\n",
            "90th percentile: 57.42%\n",
            "\n",
            "Detailed comparison saved to 'expense_comparison_results.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "def compare_expenses(predictions_csv, actual_csv, \n",
        "                     pred_col='predicted_expense', actual_col='TotalExpense',\n",
        "                     join_col='HH_ID'):\n",
        "    \"\"\"\n",
        "    Compare predicted expenses with actual expenses and calculate metrics.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    predictions_csv : str\n",
        "        Path to CSV containing predictions\n",
        "    actual_csv : str\n",
        "        Path to CSV containing actual values\n",
        "    pred_col : str\n",
        "        Column name for predicted values\n",
        "    actual_col : str\n",
        "        Column name for actual values\n",
        "    join_col : str\n",
        "        Column name to join the datasets on (usually household ID)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    # Load the datasets\n",
        "    pred_df = pd.read_csv(predictions_csv)\n",
        "    actual_df = pd.read_csv(actual_csv)\n",
        "    \n",
        "    # Check if the datasets have the required columns\n",
        "    required_cols = {\n",
        "        'predictions': [pred_col, join_col],\n",
        "        'actual': [actual_col, join_col]\n",
        "    }\n",
        "    \n",
        "    for df_name, cols in required_cols.items():\n",
        "        df = pred_df if df_name == 'predictions' else actual_df\n",
        "        missing = [col for col in cols if col not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"{df_name} dataset is missing columns: {missing}\")\n",
        "    \n",
        "    # If we have multiple rows per household, keep only one row per household in each dataset\n",
        "    pred_unique = pred_df.drop_duplicates(subset=[join_col])[[join_col, pred_col]]\n",
        "    actual_unique = actual_df.drop_duplicates(subset=[join_col])[[join_col, actual_col]]\n",
        "    \n",
        "    # Merge the datasets\n",
        "    merged_df = pd.merge(pred_unique, actual_unique, on=join_col, how='inner')\n",
        "    \n",
        "    # Print the number of households we can compare\n",
        "    print(f\"Number of households for comparison: {len(merged_df)}\")\n",
        "    \n",
        "    # Extract the values for comparison\n",
        "    y_pred = merged_df[pred_col].values\n",
        "    y_true = merged_df[actual_col].values\n",
        "    \n",
        "    # Calculate metrics\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    \n",
        "    # Calculate Mean Percentage Error (avoiding division by zero)\n",
        "    non_zero_mask = y_true != 0\n",
        "    if np.any(non_zero_mask):\n",
        "        mpe = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        "    else:\n",
        "        mpe = np.nan\n",
        "    \n",
        "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(1e-8, np.abs(y_true)))) * 100\n",
        "    \n",
        "    # Display metrics\n",
        "    print(\"\\n----- Evaluation Metrics -----\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Mean Percentage Error (MPE): {mpe:.2f}%\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "    \n",
        "    # Create a histogram of errors\n",
        "    errors = y_pred - y_true\n",
        "    percentage_errors = (errors / np.maximum(1e-8, np.abs(y_true))) * 100\n",
        "    \n",
        "    # Display summary statistics of the errors\n",
        "    print(\"\\n----- Error Distribution -----\")\n",
        "    print(f\"Min Error: {np.min(errors):.2f}\")\n",
        "    print(f\"Max Error: {np.max(errors):.2f}\")\n",
        "    print(f\"Mean Error: {np.mean(errors):.2f}\")\n",
        "    print(f\"Median Error: {np.median(errors):.2f}\")\n",
        "    \n",
        "    # Display quantiles of percentage errors\n",
        "    print(\"\\n----- Percentage Error Quantiles -----\")\n",
        "    quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
        "    for q in quantiles:\n",
        "        print(f\"{int(q*100)}th percentile: {np.percentile(percentage_errors, q*100):.2f}%\")\n",
        "    \n",
        "    # Optional: Save the comparison to a new CSV for further analysis\n",
        "    merged_df['error'] = errors\n",
        "    merged_df['percentage_error'] = percentage_errors\n",
        "    merged_df.to_csv('expense_comparison_results.csv', index=False)\n",
        "    print(\"\\nDetailed comparison saved to 'expense_comparison_results.csv'\")\n",
        "    \n",
        "    # Return the metrics as a dictionary\n",
        "    return {\n",
        "        'r2': r2,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'mpe': mpe,\n",
        "        'mape': mape\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "# Replace these paths with your actual CSV file paths\n",
        "if __name__ == \"__main__\":\n",
        "    predictions_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/test_data_with_household_sums.csv'  # Your predictions CSV\n",
        "    actual_path = '/Users/rishav/Downloads/IIT_GN/Train_Data/test_data_final.csv'  # Your actual values CSV\n",
        "    \n",
        "    # Run the comparison\n",
        "    metrics = compare_expenses(\n",
        "        predictions_csv=predictions_path,\n",
        "        actual_csv=actual_path,\n",
        "        pred_col='predicted_expense',  # Column with predictions\n",
        "        actual_col='TotalExpense',     # Column with actual values\n",
        "        join_col='HH_ID'               # Column to join on\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
